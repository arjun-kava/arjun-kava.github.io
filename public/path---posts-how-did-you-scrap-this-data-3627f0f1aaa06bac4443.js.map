{"version":3,"sources":["webpack:///path---posts-how-did-you-scrap-this-data-3627f0f1aaa06bac4443.js","webpack:///./.cache/json/posts-how-did-you-scrap-this-data.json"],"names":["webpackJsonp","434","module","exports","data","site","siteMetadata","title","subtitle","copyright","author","name","twitter","disqusShortname","url","markdownRemark","id","html","fields","tagSlugs","frontmatter","tags","date","description","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,qBAAAC,SAAA,yHAAAC,UAAA,yBAAAC,QAAgNC,KAAA,aAAAC,QAAA,cAA2CC,gBAAA,aAAAC,IAAA,kCAAuEC,gBAAmBC,GAAA,0JAAAC,KAAA;AAA4v8BC,QAA+5CC,UAAA,2IAAsJC,aAAgBb,MAAA,+BAAAc,MAAA,gGAAAC,KAAA,2BAAAC,YAAA,kPAAgbC,aAAgBC,KAAA","file":"path---posts-how-did-you-scrap-this-data-3627f0f1aaa06bac4443.js","sourcesContent":["webpackJsonp([175055536893093],{\n\n/***/ 434:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Blog by Arjun Kava\",\"subtitle\":\"I am a tireless seeker of knowledge, occasional purveyor of wisdom and also, coincidentally, a deep learning engineer.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Arjun Kava\",\"twitter\":\"arjun_kava\"},\"disqusShortname\":\"arjun-kava\",\"url\":\"https://arjun-kava.github.io/\"}},\"markdownRemark\":{\"id\":\"/Volumes/multimedia/Personal-Repositories/arjunkava-blog/src/pages/articles/2018-10-15-How-to-scrap-dataset/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<blockquote>\\n<p>Do you just tired of doing manual stuff? Does your friends are watching some sci-fi movies and are you stuck with downloading images for your next project?\\nIf everything above asked has a positive direction. You are at the right place!</p>\\n</blockquote>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-91196.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 960px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 53.76344086021505%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsSAAALEgHS3X78AAABhUlEQVQoz21Su6rCQBANfoel4g/YCNYKgh8gWJkf0A8QP0ArK/FWdoKWosFKKytBsRDBd6MmavCBj7iPe5L1hnjjYdlMdufMnJlZiVsghDwej+v1ip1Syh1gnFNOCSOEU4O9qHnwhiQ+z+fzfr/fbjdE4S5QRrEYwK3NScY/OC8LbqbwVhQlEom0220z1p80yem3XC6n06mu64ilW0AVOK9UKoFAwOPxJBKJ8Xhs8yUReL1e5/P5bDZbLBYnk8lisZjNZvP5/HQ64TYcDgeDwWQyGYvFSqWS6NE7M8JkMhmv1xuPxwuFAhRWq1Vk2G63l8sFtQyHw1wul5Lln3L5cDjYtZhkyItGoz6fLxQK1Wq11WrVbDYbjQYy73Y7VVXhMxqN6vX6fr+3mW8y+pxOp/1+vyzLSDIYDPr9PmRvNpter6dqql2kaPtHt8/nMwitVgupNE3rdDowIA8Sut3u8Xg0i6QEyxwYd43KMAxhoEPosBg7Tr4O7/8jAQ3CGPt4A077K34BPSNGUO4+qXAAAAAASUVORK5CYII='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"web-scrapping.png\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-c83f1.png\\\"\\n        srcset=\\\"/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-569e3.png 240w,\\n/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-93400.png 480w,\\n/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-c83f1.png 960w,\\n/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-23e13.png 1440w,\\n/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-91196.png 1488w\\\"\\n        sizes=\\\"(max-width: 960px) 100vw, 960px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<hr>\\n<blockquote>\\n<p>If you just want to explore code, follow git repo\\n<a href=\\\"https://github.com/zujoio/web-scrapper-python\\\">web-scrapper-python</a>\\notherwise, go ahead!</p>\\n</blockquote>\\n<hr>\\n<p><em>The Deep Learning</em> has become the most promising method for solving <em>real-world problems</em>. It is the most revolutionized innovation for <em>machine learning problems</em>.</p>\\n<p>The <em>Deep Learning Algorithms</em> have been working so well needs lots of data. The large-scale annotated datasets have been explored for the same purpose. The more annotated data we have, the better our model performs.</p>\\n<hr>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-706b5.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 960px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 45.3125%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsSAAALEgHS3X78AAABGElEQVQoz31RCY6DMAzk/x9soeFqmyopNEAOwDnX9FjtbqudWJYdZTSeOEsphRDSG2J8xj/IkLksyzvzGwBeKfOovffWWgCI9xcZNtM0/aFpnTiPTRsZT5QueV4556SUnHF6Pp+OJznJjYzMumm24V9yerY56VuWjteoIPmIgk4IUSJIWZUVIWS32yExG4So63rTDB40dOeVFj0rqlS3qSzSgQTeWW1eFoAxlu/3lFKlVDYbI4YhBTcRft33wEXSCueONxH7W8I8jv7+KQ+fmIP3T88hRAfrXDN2sU/bPyy8t79+G681vZwOwwzOwmq0lnIax9EYA+uKsS4L1p/J2wIcHh+8Q4Gu65qmadsWveFs/g7cSPwk/gVh2QkeC/jeSgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"data-need-graph.png\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-c83f1.png\\\"\\n        srcset=\\\"/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-569e3.png 240w,\\n/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-93400.png 480w,\\n/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-c83f1.png 960w,\\n/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-23e13.png 1440w,\\n/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-706b5.png 1600w\\\"\\n        sizes=\\\"(max-width: 960px) 100vw, 960px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p>But well gathering well-annotated data can be time-consuming to acquire as well as expensive. The process can be automated compared to a traditional manual method using web-scrapping.</p>\\n<p>Today’s article is all about web-scrapping using Python.</p>\\n<hr>\\n<h2>HTML Introduction</h2>\\n<p>I am assuming you know basic HTML tags but If my assumption is False then follow the link <a href=\\\"https://html.com/\\\">HTML Tutorial</a>. </p>\\n<p>Anyways, You have been realized that HTML tags have attributes such as <code class=\\\"language-text\\\">class</code> and <code class=\\\"language-text\\\">id</code> which can be very useful to locate unstructured data of the website.</p>\\n<p>The same HTML properties and tags will be used to gather information about structure of data while scrapping.</p>\\n<hr>\\n<h2>Advice</h2>\\n<p>Few pieces of advice before starting:</p>\\n<ol>\\n<li>Respect the Terms of Service (ToS) for commercial purpose.</li>\\n<li>Don’t republish your scraped data or any derivative dataset without verifying the license of the data, or without obtaining a written permission from the copyright holder.</li>\\n<li>Don’t base your whole business on data scraping. The website(s) that you scrape may eventually block you.</li>\\n</ol>\\n<p>for more information, I recommended reading the great article <a href=\\\"https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/\\\">Web Scraping and Crawling Are Perfectly Legal, Right?</a> by <a href=\\\"https://benbernardblog.com/about-me/\\\">Ben Bernard</a></p>\\n<hr>\\n<p>I am going to explain web scrapping implementation step by step in following sections, also the roadmap is list down as below:</p>\\n<ol>\\n<li>Getting Started</li>\\n<li>Web Inspector</li>\\n<li>Getting Start with Code</li>\\n<li>Advanced Usage with Selenium Web Driver</li>\\n<li>Advanced Usage with Threading</li>\\n<li>Advanced Usage with Scrapy</li>\\n<li>References</li>\\n</ol>\\n<p>Excited? Let’s start step by step.</p>\\n<hr>\\n<h2>Getting Started</h2>\\n<p>We are going to use powerful and simple Python as our scrapping language. </p>\\n<ul>\\n<li>For installing Python, download compatible version of Python from <a href=\\\"https://www.python.org/downloads/\\\">https://www.python.org/downloads/</a></li>\\n</ul>\\n<p>further, we need to install <a href=\\\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\\\"><code class=\\\"language-text\\\">BeautifulSoup</code></a> and <a href=\\\"https://pypi.org/project/cssutils/\\\"><code class=\\\"language-text\\\">cssutils</code></a> using <code class=\\\"language-text\\\">pip</code>, a package management tool for Python.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">pip install BeautifulSoup\\npip install cssutils</code></pre>\\n      </div>\\n<p>If you are failed to execute commands add <code class=\\\"language-text\\\">--user</code> at end of each or try with <code class=\\\"language-text\\\">sudo</code></p>\\n<hr>\\n<h2>Web Inspector</h2>\\n<p>Web Inspector is a tool which is going to help us to find HTML behind the scene of any particular website.</p>\\n<p>Let’s take the example of  <a href=\\\"burst.shopify.com\\\">burst.shopify.com</a> page. Open page and select category Dog then right-click on the image. It will open your browser’s inspector to inspect a webpage. As shown below:</p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-968cc.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 855px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 52.046783625730995%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAAC4ElEQVQozyXR6UvTARzH8f0FFWo3brbsmFet1Kl55lyTZjIzkDITK51Wmq1sR5ZT/Gn3ujwwnJW11GxzWqG5ooNo1YOiCLLDrIQMTMigR/ruRz34Pvk+eH0+X74SrWY9ujQNqWviKdu9i5KinSxYGEhgYBDZWXrUKcnIZEH4+wcgDZKhF3e5W3IJDQlBKpOSkJyEPnsT+pxckjRaJOGKUJQKBTFKJRv1WWzYkElAwBz8/GcTFRmJcsUK5s6dh5+fH9JFclLUYnhqGkuXhzBr5kzkcilxCYkkrFWzRHQkqhiVmJqFTqdDq9Wi0WhENIN07TqSEuJRr03510adGIepfDc5mTpyN+dw8MB+LGYTOwvyKd6RT4ZWTUSICAqCgM/nwzs4iNfr5e5AP/13buNxu7hx/Rp3et3kb8tDn56KveYQdWUlWIx7qT1soa7KSrW5nKM2M8eNxaTGRiFxOp1MTU0xPj7+b36MjTEy/JGRj0N8/vCOr5/e09xwHqtxD1VWK/ZT57jY0MTZkycQrEZqKk20t12isWIX2njVf3B6epqJiQl+iuCv3394POCh+ZiNb19GGHr7moG+Hjw3uvC4PDg7unG7e3n+4iX1tQKOszVcOS1wRmyaFCuCDoeDyclJRkdH+f59jOGhN/S3HKGzQWB4+DO+xw9wd3Xi6uzkVm8fjx485P69+/iePiO/oJCYmDWUFWwjQ50iPmgxEoPBQEtLC+cvNNB4zk5pfSvZx/opMJ0SzzFjq7Rws6OZbmcbPR2X6eu+ive2ixdP7tHuaKTQUMSO7EzKirazZWsekuDgYMLCwlCILw8PVbBsZSzy6PXIw6MIlssIi1BS2+rC3u7laNsgda13sTsf0dTzir3l+0hLjCZZFUlyXDRK5UokcapVVFtKOSlYqK4yU2k9gGmPkSqTDfPBCkymCgz7bOSV1lFsu0ax0EWhcJOS+m4SM0sIly4gPTqCjNVBLJs/g79sC9olDEVePQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"inspect-element.png\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-968cc.png\\\"\\n        srcset=\\\"/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-1da2c.png 240w,\\n/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-48905.png 480w,\\n/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-968cc.png 855w\\\"\\n        sizes=\\\"(max-width: 855px) 100vw, 855px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    \\n\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-23e13.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 960px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 46.18055555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAACUklEQVQozz2Sy08TURSH+xe4dqdGNrqVjQsMweiOpcZXwLhQIQp9IJSqNATQjUEXYqKLSly4NYqCBhOMJhpFpKIJChTa0mk7nWenM1Pog8/bMeEmJ/fe3Hu+8/r5JEnCNE1KpRKapqEqKrquo6oKilIQbwaaqtL4pxQ0cvnGexFdMzDEf9MwhK9N2XVwHBvfwMAAqVQK27Y9oFIokMtmSCfX2Vj9S2ZjjbnZGaLRKDXHJZsuIMsWqiyCygImgpedLWqVimc+v99PMpkUdGcXmM9KyPkc6fU1Dzg7PcVgZJCdap1MSqagWCKBLcrlCvV6nZ0dvNXYfMFgcBfYKLUBlTIZXj6Psfj1C0pO4umjB/T3BT0nu5BHV4o4lkvZcqhtbUOtSr1S9ZC+QCBAIpH430PRK9Uokv6zwJPbl1n8Po8sgPeiYXr9QdZt+LZqsLCis5B0iaddFlMuy7kyy/ltlgtVfKFQyCuvQa+LnEuqxNzkCG9jd4h//sCvH/OM9l3huj/EkgGf1lzeL+lMx01hOjNxlXc/NWZ/W3xMbOO72tVFbPIZU6+nefHqDbHHDxkOBxgdu0tkMELvtW7aTxyj/0afV3JRTD+zkUFJZzGkLKYkzpsSm0mZkiGmfOrsBfY2t9PU1sG+ljPsbznN4ZOdHGw9x4HW8zQd72TPoTZ6/AFsp+z12CqaWFYJQ0hG9eQj5CUnsYs6vksXOxgKhxi7GaGnq5vx8ftMTEwwHB1iZOgW4WAvR5uPEBLDs4XO0lIOWdFwXde720JKpiKyza6I7LP8A25MWfR/yO4fAAAAAElFTkSuQmCC'); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"inspect-element-html.png\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-c83f1.png\\\"\\n        srcset=\\\"/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-569e3.png 240w,\\n/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-93400.png 480w,\\n/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-c83f1.png 960w,\\n/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-23e13.png 1440w\\\"\\n        sizes=\\\"(max-width: 960px) 100vw, 960px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    \\n<em>- Web Inspector Example</em></p>\\n<p>It is going to display the source of code which might have a structure such as follow:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&lt;div class=&quot;tile tile--with-overlay gutter-bottom&quot;&gt;\\n    &lt;div class=&quot;photo-tile&quot;&gt;\\n        &lt;a class=&quot;photo-tile__image-wrapper&quot;&gt;\\n            &lt;img sizes=&quot;100vw&quot; data-srcset=&quot;https:...&quot; \\n                 class&quot;tile__image js-track-photo-stat-view js-track-photo-stat-click lazyloaded&quot;&gt;</code></pre>\\n      </div>\\n<p>The HTML tags are uniquely identified by properties of tags. As a result, now, We know the location of uniquely identified data during the process of scrapping.</p>\\n<hr>\\n<h2>Getting Start with Code</h2>\\n<p>The Following is configuration need to be placed into <code class=\\\"language-text\\\">config.py</code> file. This global configuration is going to be used in different scripts.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Target dataset path\\nDATASET_PATH = &quot;./dataset&quot;\\n\\n# Fake user agent for avoiding 503 error\\nHEADERS = {\\n    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36&#39;\\n}\\n\\n# Base url of scrapping\\nBASE_URL = &quot;https://burst.shopify.com&quot;\\n\\n# Advanced parameters\\n# Categories want to scrap\\nCATEGORIES = [&quot;dog&quot;,&quot;cat&quot;]\\n\\n# Page limit to search images from URL\\nPAGE_FROM =1\\nPAGE_TO = 2\\n\\n# Number of workers for downloading pages and images for better and faster performance\\nWORKERS = 4</code></pre>\\n      </div>\\n<p>Source: <em><a href=\\\"https://github.com/zujoio/web-scrapper-python/blob/master/config.py\\\">config.py</a></em></p>\\n<p>Now, Let’s get our hand dirty with code. import some libraries to getting started with actual code:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">from bs4 import BeautifulSoup\\nimport os\\nimport urllib.request\\nfrom tqdm import tqdm\\nimport ssl</code></pre>\\n      </div>\\n<p><code class=\\\"language-text\\\">BeautifulSoup</code> is used for scraping web pages and images while <code class=\\\"language-text\\\">urllib</code> is imported to download web pages and images. <code class=\\\"language-text\\\">tqdm</code> is used for just displaying progress and <code class=\\\"language-text\\\">ssl</code> used for creating fake verification of request.</p>\\n<p>also, we need to import <code class=\\\"language-text\\\">config.py</code> file to use global configuration.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">from config import *</code></pre>\\n      </div>\\n<p>Before starting I am putting some local configuration which is going to used as below.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">timeout = 60 # Request timeout\\nurl = BASE_URL + &quot;/dog&quot; # URL being scrapped\\ntarget_dir = os.path.join(DATASET_PATH,&quot;dog&quot;) # Target directory for scrapping data</code></pre>\\n      </div>\\n<p>To download source of the page, I have used <code class=\\\"language-text\\\">urllib</code> library where <code class=\\\"language-text\\\">context</code> specifies a fake SSL certificate to avoid SSL Exceptions and <code class=\\\"language-text\\\">HEADERS</code> are imported from global configuration to avoid 503 Exception generated by the web servers.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Bypass SSL verification\\ncontext = ssl._create_unverified_context()\\n\\n# Read HTML page and save as long string\\nreq = urllib.request.Request(url, headers=HEADERS)\\nresponse = urllib.request.urlopen(req, timeout=timeout, context=context)\\n\\n# Read page source\\nhtml = response.read()</code></pre>\\n      </div>\\n<p>The downloaded HTML source code needs to be parsed to access properties of HTML tags. <code class=\\\"language-text\\\">BeautifulSoup</code> is going to use for the same context. It has well-optimized classes and methods to access HTML tags by their unique properties.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Parse HTML source using BeautifulSoup\\nsoup =  BeautifulSoup(html, &quot;html.parser&quot;)</code></pre>\\n      </div>\\n<p>Now we have a handler called <code class=\\\"language-text\\\">soup</code> which is the parsed version of HTML source code that can be attached to any supported method of <code class=\\\"language-text\\\">BeautifulSoup</code> class.</p>\\n<p>As we had learned earlier about unique tags of data. <code class=\\\"language-text\\\">soup</code> has a <code class=\\\"language-text\\\">select()</code> method which is going to help us to find specific tags from parsed HTML content. Since we have unique class of <code class=\\\"language-text\\\">&lt;Img class=&quot;js-track-photo-stat-view&quot; &gt;</code> tag.</p>\\n<p>Below piece of code is will fetch all <code class=\\\"language-text\\\">&lt;Img&gt;</code> tags from a page whose class defined as <code class=\\\"language-text\\\">js-track-photo-stat-view</code>. </p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">image_grids = soup.select(&#39;.js-track-photo-stat-view&#39;)</code></pre>\\n      </div>\\n<p>Next, we have to extract URL of images which can be helpful to download the image.</p>\\n<p><code class=\\\"language-text\\\">image_grids</code> includes multiple entries of <code class=\\\"language-text\\\">&lt;Img&gt;</code> as shown listed below. As you can see, <code class=\\\"language-text\\\">&lt;Img&gt;</code> tag has property named <code class=\\\"language-text\\\">data-srcset</code> which includes URLs of images as per resolution 1x, 2x and so on.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&lt;img alt=&quot;good morning sunshine&quot; \\nclass=&quot;tile__image js-track-photo-stat-view js-track-photo-stat-click lazyload&quot; \\ndata-category-handle=&quot;dog&quot; \\ndata-photo-id=&quot;1394&quot; \\ndata-photo-title=&quot;Good Morning Sunshine&quot; \\ndata-srcset=&quot;https://burst.shopifycdn.com/photos/good-morning-sunshine_373x.progressive.jpg 1x, https://burst.shopifycdn.com/photos/good-morning-sunshine_373x@2x.progressive.jpg 2x&quot; \\nsizes=&quot;100vw&quot; \\nsrc=&quot;https://burst.shopifycdn.com/photos/good-morning-sunshine_70x.progressive.jpg&quot;/&gt;</code></pre>\\n      </div>\\n<p>We can access properties of <code class=\\\"language-text\\\">&lt;Img&gt;</code> tag using <code class=\\\"language-text\\\">get()</code> method. Below code going to fetch the content of <code class=\\\"language-text\\\">data-srcset</code> property as well as preprocess data to find the highest resolution image from it.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">image_urls = []\\nfor image_tag in tqdm(image_grids,desc=&quot;Find Images&quot;):\\n\\n    # Fetch data tag which includes sequence of URLS\\n    image_url = image_tag.get(&#39;data-srcset&#39;)\\n\\n    # Extract highest resolution image from data tad\\n    image_url = image_url.split(&#39;,&#39;)\\n    high_resolution_pair = image_url[-1].split(&#39; &#39;)\\n    high_resolution_image_url = high_resolution_pair[1].replace(&quot;@2x&quot;, &quot;@3x&quot;)\\n\\n    # Stack all image urls\\n    image_urls.append(high_resolution_image_url)</code></pre>\\n      </div>\\n<p>Now, We have a list of URLs stacked into <code class=\\\"language-text\\\">image_urls</code> list. All we need to do is download images into targeted directories. Below script is going to do the rest.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Download images into target directory\\nfor image_url in tqdm(image_urls,desc=&quot;Download Images&quot;):\\n\\n    # Extract name of file from URL\\n    file_name = image_url.split(&quot;/&quot;)[-1]\\n\\n    # Build target path of image\\n    image_path = os.path.join(target_dir, file_name)\\n\\n    # Create directories\\n    if not os.path.exists(target_dir): os.mkdir(target_dir)\\n\\n    # Write image to file system\\n    if not os.path.exists(image_path):\\n\\n        # Read image from web\\n        req = urllib.request.Request(image_url, headers=HEADERS)\\n        response = urllib.request.urlopen(req, timeout=timeout, context=context)\\n\\n        # Write it down to file system\\n        f = open(image_path, &#39;wb&#39;)\\n        f.write(response.read())\\n        f.close()</code></pre>\\n      </div>\\n<p>Every image fetched will be downloaded in target directory.\\nThe above script is going to displays output as below which might change as per your computer’s perrformance and speed of internet!</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">Find Images: 100%|###########################################################################################################################################| 50/50 [00:00&lt;00:00, 107933.71it/s]\\nDownload Images: 100%|###########################################################################################################################################| 50/50 [00:42&lt;00:00,  2.81it/s]</code></pre>\\n      </div>\\n<p>You can find the whole script at github repo <a href=\\\"https://github.com/zujoio/web-scrapper-python/blob/master/basic_scrapper.py\\\"><code class=\\\"language-text\\\">basic_scrapper.py</code></a></p>\\n<p>You can find other methods and use of <code class=\\\"language-text\\\">BeautifulSoap</code> at official documentation from <a href=\\\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\\\">BeautifulSoup</a></p>\\n<hr>\\n<h2>Advanced Usage with Selenium Web Driver</h2>\\n<p>The problem with simple <code class=\\\"language-text\\\">urllib</code> is that It does not support to download dynamic HTML content from specified URL.</p>\\n<p>HTML going to be load after javascript executes and dynamically generate content of the page. At this moment <code class=\\\"language-text\\\">urllib</code> going to download page source but it does not include page source which required for downloading images.</p>\\n<p>For example, open any product page from <a href=\\\"https://www.amazon.com/Old-Man-Sea-Ernest-Hemingway/dp/0684801221/ref=sr_1_1?ie=UTF8&#x26;qid=1540649353&#x26;sr=8-1&#x26;keywords=old+man+and+the+sea\\\">amazon.com</a>. The whole content of the product will be generated after the javascript done. </p>\\n<p>This javascript issue can be solved using <code class=\\\"language-text\\\">selenium</code> web driver for downloading page source. It is going to open the web page autonomously and waits until every script on the page is going to be executed.</p>\\n<p>To start with <code class=\\\"language-text\\\">selenium</code>l, you have to install specific browser web driver bindings as per your system configuration using the following link. In my case, I am going to use Firefox for Mac OS.</p>\\n<h4>Official Selenium Python Package Page:</h4>\\n<p><a href=\\\"https://pypi.org/project/selenium/\\\">https://pypi.org/project/selenium/</a></p>\\n<h4>Download Firefox Web Driver Bindings:</h4>\\n<p><a href=\\\"https://github.com/mozilla/geckodriver/releases\\\">https://github.com/mozilla/geckodriver/releases</a></p>\\n<p>Install selenium package using following script</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">pip install selenium</code></pre>\\n      </div>\\n<p>Now import web driver and replace fetching page source with below code:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">from selenium import web driver\\n\\n# Use selenium firefox driver to get page source\\nbrowser = webdriver.Firefox()\\nbrowser.get(url)\\n\\n# get page source\\nhtml = browser.page_source</code></pre>\\n      </div>\\n<p>It will open an autonomous firefox window and close it after the whole page loaded successfully. You can find more use case of <code class=\\\"language-text\\\">selenium</code> from<a href=\\\"https://selenium-python.readthedocs.io/\\\">selenium-python.readthedocs.io/</a></p>\\n<hr>\\n<h2>Advanced Usage with Threading</h2>\\n<p>The above code is basic so It is not going to utilize resources available. To overcome the problem, Threading can be used to improve the performance of scrapper. Web pages and images can be downloaded parallelly using <code class=\\\"language-text\\\">ThreadPoolExecutor</code> of inbuild <code class=\\\"language-text\\\">concurrent</code> package of python.</p>\\n<p>Bolow code downloads images with array of <code class=\\\"language-text\\\">images_urls</code> using multi-threading. <code class=\\\"language-text\\\">download_image</code> is function takes single <code class=\\\"language-text\\\">url</code> of image and <code class=\\\"language-text\\\">timeout</code> of request as input. The images is going to be downloded parallely using following script.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Initialize thread pool executor\\nexecutor = concurrent.futures.ThreadPoolExecutor(max_workers=5):\\n\\n# Start the load operations and mark each future with its URL\\nfuture_to_url = {executor.submit(download_image, url, timeout): url for url in image_urls}\\ncount_images = 0\\nfor future in concurrent.futures.as_completed(future_to_url):\\n    files = future.result()\\n    count_images = count_images + len(files)</code></pre>\\n      </div>\\n<p>Find out more about <code class=\\\"language-text\\\">ThreadPoolExecutor</code> from <a href=\\\"https://docs.python.org/3/library/concurrent.futures.html\\\">concurrent.futures.html</a></p>\\n<hr>\\n<h1>Advanced Usage with Scrapy</h1>\\n<p><code class=\\\"language-text\\\">Scrapy</code> can be used as advancement in scrapping. It contains rich classes and methods to extract data from multiple sources. </p>\\n<p>First of all, Install scrappy by using the following command:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">pip install scrapy</code></pre>\\n      </div>\\n<p><code class=\\\"language-text\\\">Spiders</code> are classes that you define and that <code class=\\\"language-text\\\">Scrapy</code> uses to scrape information from a website (or a group of websites). They must subclass scrapy. Spider and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.</p>\\n<p>Let’s start with the simple example which extracts titles of a blog from multiple pages of content. </p>\\n<p>First of all, I have to import the scrapy library and implement class <code class=\\\"language-text\\\">BlogSpider</code> extended from <code class=\\\"language-text\\\">scrapy.Spider</code> base class. I have to declare the name of spider and URL using <code class=\\\"language-text\\\">name</code>  and  <code class=\\\"language-text\\\">start_urls</code> respectively. Show as below:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">import scrapy\\n\\n&quot;&quot;&quot;\\n    Abstraction of Blog Scrapper\\n&quot;&quot;&quot;\\nclass BlogSpider(scrapy.Spider):\\n    # configure name of blog spider and URL\\n    name = &#39;blogspider&#39;\\n    start_urls = [&#39;https://blog.scrapinghub.com&#39;]</code></pre>\\n      </div>\\n<p>Now, I have to override <code class=\\\"language-text\\\">parse</code> method to scrap data from the specified URL. Below code is going to find <code class=\\\"language-text\\\">&lt;h2&gt;</code> from the page whose class is <code class=\\\"language-text\\\">post-header</code> then extract the text of the first element. </p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&quot;&quot;&quot;\\n    Implement parser \\n&quot;&quot;&quot;\\ndef parse(self, response):\\n    # extract titles from page\\n    for title in response.css(&#39;.post-header&gt;h2&#39;):\\n        yield {&#39;title&#39;: title.css(&#39;a ::text&#39;).extract_first()}</code></pre>\\n      </div>\\n<p>After that, I have called <code class=\\\"language-text\\\">parse</code> method recursively for each page in defined URL. It is much easier to work with <code class=\\\"language-text\\\">Scrapy</code> as well as much shorter code requires comparatively.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">def parse(self, response):\\n    # extract titles from page\\n    for title in response.css(&#39;.post-header&gt;h2&#39;):\\n        yield {&#39;title&#39;: title.css(&#39;a ::text&#39;).extract_first()}\\n\\n    # move to the next page\\n    for next_page in response.css(&#39;div.prev-post &gt; a&#39;):\\n        yield response.follow(next_page, self.parse)</code></pre>\\n      </div>\\n<p>The whole code is defined as below:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">import scrapy\\n\\n&quot;&quot;&quot;\\n    Abstraction of Blog Scrapper\\n&quot;&quot;&quot;\\nclass BlogSpider(scrapy.Spider):\\n    # configure name of blog spider and URL\\n    name = &#39;blogspider&#39;\\n    start_urls = [&#39;https://blog.scrapinghub.com&#39;]\\n\\n    &quot;&quot;&quot;\\n        Implement parser \\n    &quot;&quot;&quot;\\n    def parse(self, response):\\n        # extract titles from page\\n        for title in response.css(&#39;.post-header&gt;h2&#39;):\\n            yield {&#39;title&#39;: title.css(&#39;a ::text&#39;).extract_first()}\\n\\n        # move to the next page\\n        for next_page in response.css(&#39;div.prev-post &gt; a&#39;):\\n            yield response.follow(next_page, self.parse)</code></pre>\\n      </div>\\n<p>Find out more about <code class=\\\"language-text\\\">Scrapy</code> at official documentation link: <a href=\\\"https://scrapy.org/\\\">https://scrapy.org/</a></p>\\n<hr>\\n<p>Guys, that’s all from my end. You can find out code from the following repository.</p>\\n<p><a href=\\\"https://github.com/zujoio/web-scrapper-python\\\">https://github.com/zujoio/web-scrapper-python</a></p>\\n<p>Hope you like the article. If you have any questions, please feel free to leave a comment below.</p>\\n<h2>References</h2>\\n<ol>\\n<li><a href=\\\"https://www.datacamp.com/community/tutorials/web-scraping-using-python\\\">https://www.datacamp.com/community/tutorials/web-scraping-using-python</a></li>\\n<li><a href=\\\"https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/\\\">https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/</a></li>\\n<li><a href=\\\"https://docs.python.org/dev/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor\\\">https://docs.python.org/dev/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor</a></li>\\n<li><a href=\\\"https://scrapy.org/resources/\\\">https://scrapy.org/resources/</a></li>\\n<li><a href=\\\"https://docs.scrapy.org/en/latest/intro/tutorial.html\\\">https://docs.scrapy.org/en/latest/intro/tutorial.html</a></li>\\n</ol>\",\"fields\":{\"tagSlugs\":[\"/tags/web-scrapping/\",\"/tags/python/\",\"/tags/deep-learning/\",\"/tags/neural-network/\",\"/tags/machine-learning/\",\"/tags/computer-vision/\"]},\"frontmatter\":{\"title\":\"How did you scrap this data?\",\"tags\":[\"web scrapping\",\"python\",\"deep learning\",\"neural network\",\"machine learning\",\"computer vision\"],\"date\":\"2018-10-15T12:21:32.381Z\",\"description\":\"Do you just tired of doing manual stuff? Does your friends are watching some sci-fi movies and are you stuck with downloading images for your next project?  If everything above asked has a positive direction. You are at the right place!\"}}},\"pathContext\":{\"slug\":\"/posts/how-did-you-scrap-this-data/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---posts-how-did-you-scrap-this-data-3627f0f1aaa06bac4443.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Blog by Arjun Kava\",\"subtitle\":\"I am a tireless seeker of knowledge, occasional purveyor of wisdom and also, coincidentally, a deep learning engineer.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Arjun Kava\",\"twitter\":\"arjun_kava\"},\"disqusShortname\":\"arjun-kava\",\"url\":\"https://arjun-kava.github.io/\"}},\"markdownRemark\":{\"id\":\"/Volumes/multimedia/Personal-Repositories/arjunkava-blog/src/pages/articles/2018-10-15-How-to-scrap-dataset/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<blockquote>\\n<p>Do you just tired of doing manual stuff? Does your friends are watching some sci-fi movies and are you stuck with downloading images for your next project?\\nIf everything above asked has a positive direction. You are at the right place!</p>\\n</blockquote>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-91196.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 960px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 53.76344086021505%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAALCAIAAADwazoUAAAACXBIWXMAAAsSAAALEgHS3X78AAABhUlEQVQoz21Su6rCQBANfoel4g/YCNYKgh8gWJkf0A8QP0ArK/FWdoKWosFKKytBsRDBd6MmavCBj7iPe5L1hnjjYdlMdufMnJlZiVsghDwej+v1ip1Syh1gnFNOCSOEU4O9qHnwhiQ+z+fzfr/fbjdE4S5QRrEYwK3NScY/OC8LbqbwVhQlEom0220z1p80yem3XC6n06mu64ilW0AVOK9UKoFAwOPxJBKJ8Xhs8yUReL1e5/P5bDZbLBYnk8lisZjNZvP5/HQ64TYcDgeDwWQyGYvFSqWS6NE7M8JkMhmv1xuPxwuFAhRWq1Vk2G63l8sFtQyHw1wul5Lln3L5cDjYtZhkyItGoz6fLxQK1Wq11WrVbDYbjQYy73Y7VVXhMxqN6vX6fr+3mW8y+pxOp/1+vyzLSDIYDPr9PmRvNpter6dqql2kaPtHt8/nMwitVgupNE3rdDowIA8Sut3u8Xg0i6QEyxwYd43KMAxhoEPosBg7Tr4O7/8jAQ3CGPt4A077K34BPSNGUO4+qXAAAAAASUVORK5CYII='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"web-scrapping.png\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-c83f1.png\\\"\\n        srcset=\\\"/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-569e3.png 240w,\\n/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-93400.png 480w,\\n/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-c83f1.png 960w,\\n/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-23e13.png 1440w,\\n/static/web-scrapping-216b29a49bc7e04e45ec99225d4aceb6-91196.png 1488w\\\"\\n        sizes=\\\"(max-width: 960px) 100vw, 960px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<hr>\\n<blockquote>\\n<p>If you just want to explore code, follow git repo\\n<a href=\\\"https://github.com/zujoio/web-scrapper-python\\\">web-scrapper-python</a>\\notherwise, go ahead!</p>\\n</blockquote>\\n<hr>\\n<p><em>The Deep Learning</em> has become the most promising method for solving <em>real-world problems</em>. It is the most revolutionized innovation for <em>machine learning problems</em>.</p>\\n<p>The <em>Deep Learning Algorithms</em> have been working so well needs lots of data. The large-scale annotated datasets have been explored for the same purpose. The more annotated data we have, the better our model performs.</p>\\n<hr>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-706b5.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 960px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 45.3125%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAIAAAC9o5sfAAAACXBIWXMAAAsSAAALEgHS3X78AAABGElEQVQoz31RCY6DMAzk/x9soeFqmyopNEAOwDnX9FjtbqudWJYdZTSeOEsphRDSG2J8xj/IkLksyzvzGwBeKfOovffWWgCI9xcZNtM0/aFpnTiPTRsZT5QueV4556SUnHF6Pp+OJznJjYzMumm24V9yerY56VuWjteoIPmIgk4IUSJIWZUVIWS32yExG4So63rTDB40dOeVFj0rqlS3qSzSgQTeWW1eFoAxlu/3lFKlVDYbI4YhBTcRft33wEXSCueONxH7W8I8jv7+KQ+fmIP3T88hRAfrXDN2sU/bPyy8t79+G681vZwOwwzOwmq0lnIax9EYA+uKsS4L1p/J2wIcHh+8Q4Gu65qmadsWveFs/g7cSPwk/gVh2QkeC/jeSgAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"data-need-graph.png\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-c83f1.png\\\"\\n        srcset=\\\"/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-569e3.png 240w,\\n/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-93400.png 480w,\\n/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-c83f1.png 960w,\\n/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-23e13.png 1440w,\\n/static/data-need-graph-1536d3037f2efdf436a796d76d717d8b-706b5.png 1600w\\\"\\n        sizes=\\\"(max-width: 960px) 100vw, 960px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    </p>\\n<p>But well gathering well-annotated data can be time-consuming to acquire as well as expensive. The process can be automated compared to a traditional manual method using web-scrapping.</p>\\n<p>Today’s article is all about web-scrapping using Python.</p>\\n<hr>\\n<h2>HTML Introduction</h2>\\n<p>I am assuming you know basic HTML tags but If my assumption is False then follow the link <a href=\\\"https://html.com/\\\">HTML Tutorial</a>. </p>\\n<p>Anyways, You have been realized that HTML tags have attributes such as <code class=\\\"language-text\\\">class</code> and <code class=\\\"language-text\\\">id</code> which can be very useful to locate unstructured data of the website.</p>\\n<p>The same HTML properties and tags will be used to gather information about structure of data while scrapping.</p>\\n<hr>\\n<h2>Advice</h2>\\n<p>Few pieces of advice before starting:</p>\\n<ol>\\n<li>Respect the Terms of Service (ToS) for commercial purpose.</li>\\n<li>Don’t republish your scraped data or any derivative dataset without verifying the license of the data, or without obtaining a written permission from the copyright holder.</li>\\n<li>Don’t base your whole business on data scraping. The website(s) that you scrape may eventually block you.</li>\\n</ol>\\n<p>for more information, I recommended reading the great article <a href=\\\"https://benbernardblog.com/web-scraping-and-crawling-are-perfectly-legal-right/\\\">Web Scraping and Crawling Are Perfectly Legal, Right?</a> by <a href=\\\"https://benbernardblog.com/about-me/\\\">Ben Bernard</a></p>\\n<hr>\\n<p>I am going to explain web scrapping implementation step by step in following sections, also the roadmap is list down as below:</p>\\n<ol>\\n<li>Getting Started</li>\\n<li>Web Inspector</li>\\n<li>Getting Start with Code</li>\\n<li>Advanced Usage with Selenium Web Driver</li>\\n<li>Advanced Usage with Threading</li>\\n<li>Advanced Usage with Scrapy</li>\\n<li>References</li>\\n</ol>\\n<p>Excited? Let’s start step by step.</p>\\n<hr>\\n<h2>Getting Started</h2>\\n<p>We are going to use powerful and simple Python as our scrapping language. </p>\\n<ul>\\n<li>For installing Python, download compatible version of Python from <a href=\\\"https://www.python.org/downloads/\\\">https://www.python.org/downloads/</a></li>\\n</ul>\\n<p>further, we need to install <a href=\\\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\\\"><code class=\\\"language-text\\\">BeautifulSoup</code></a> and <a href=\\\"https://pypi.org/project/cssutils/\\\"><code class=\\\"language-text\\\">cssutils</code></a> using <code class=\\\"language-text\\\">pip</code>, a package management tool for Python.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">pip install BeautifulSoup\\npip install cssutils</code></pre>\\n      </div>\\n<p>If you are failed to execute commands add <code class=\\\"language-text\\\">--user</code> at end of each or try with <code class=\\\"language-text\\\">sudo</code></p>\\n<hr>\\n<h2>Web Inspector</h2>\\n<p>Web Inspector is a tool which is going to help us to find HTML behind the scene of any particular website.</p>\\n<p>Let’s take the example of  <a href=\\\"burst.shopify.com\\\">burst.shopify.com</a> page. Open page and select category Dog then right-click on the image. It will open your browser’s inspector to inspect a webpage. As shown below:</p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-968cc.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 855px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 52.046783625730995%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAKCAYAAAC0VX7mAAAACXBIWXMAAAsSAAALEgHS3X78AAAC4ElEQVQozyXR6UvTARzH8f0FFWo3brbsmFet1Kl55lyTZjIzkDITK51Wmq1sR5ZT/Gn3ujwwnJW11GxzWqG5ooNo1YOiCLLDrIQMTMigR/ruRz34Pvk+eH0+X74SrWY9ujQNqWviKdu9i5KinSxYGEhgYBDZWXrUKcnIZEH4+wcgDZKhF3e5W3IJDQlBKpOSkJyEPnsT+pxckjRaJOGKUJQKBTFKJRv1WWzYkElAwBz8/GcTFRmJcsUK5s6dh5+fH9JFclLUYnhqGkuXhzBr5kzkcilxCYkkrFWzRHQkqhiVmJqFTqdDq9Wi0WhENIN07TqSEuJRr03510adGIepfDc5mTpyN+dw8MB+LGYTOwvyKd6RT4ZWTUSICAqCgM/nwzs4iNfr5e5AP/13buNxu7hx/Rp3et3kb8tDn56KveYQdWUlWIx7qT1soa7KSrW5nKM2M8eNxaTGRiFxOp1MTU0xPj7+b36MjTEy/JGRj0N8/vCOr5/e09xwHqtxD1VWK/ZT57jY0MTZkycQrEZqKk20t12isWIX2njVf3B6epqJiQl+iuCv3394POCh+ZiNb19GGHr7moG+Hjw3uvC4PDg7unG7e3n+4iX1tQKOszVcOS1wRmyaFCuCDoeDyclJRkdH+f59jOGhN/S3HKGzQWB4+DO+xw9wd3Xi6uzkVm8fjx485P69+/iePiO/oJCYmDWUFWwjQ50iPmgxEoPBQEtLC+cvNNB4zk5pfSvZx/opMJ0SzzFjq7Rws6OZbmcbPR2X6eu+ive2ixdP7tHuaKTQUMSO7EzKirazZWsekuDgYMLCwlCILw8PVbBsZSzy6PXIw6MIlssIi1BS2+rC3u7laNsgda13sTsf0dTzir3l+0hLjCZZFUlyXDRK5UokcapVVFtKOSlYqK4yU2k9gGmPkSqTDfPBCkymCgz7bOSV1lFsu0ax0EWhcJOS+m4SM0sIly4gPTqCjNVBLJs/g79sC9olDEVePQAAAABJRU5ErkJggg=='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"inspect-element.png\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-968cc.png\\\"\\n        srcset=\\\"/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-1da2c.png 240w,\\n/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-48905.png 480w,\\n/static/inspect-element-0bdc93fd24313edf657eb9d25f2a87e3-968cc.png 855w\\\"\\n        sizes=\\\"(max-width: 855px) 100vw, 855px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    \\n\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-23e13.png\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 960px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 46.18055555555556%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAJCAYAAAAywQxIAAAACXBIWXMAAAsSAAALEgHS3X78AAACUklEQVQozz2Sy08TURSH+xe4dqdGNrqVjQsMweiOpcZXwLhQIQp9IJSqNATQjUEXYqKLSly4NYqCBhOMJhpFpKIJChTa0mk7nWenM1Pog8/bMeEmJ/fe3Hu+8/r5JEnCNE1KpRKapqEqKrquo6oKilIQbwaaqtL4pxQ0cvnGexFdMzDEf9MwhK9N2XVwHBvfwMAAqVQK27Y9oFIokMtmSCfX2Vj9S2ZjjbnZGaLRKDXHJZsuIMsWqiyCygImgpedLWqVimc+v99PMpkUdGcXmM9KyPkc6fU1Dzg7PcVgZJCdap1MSqagWCKBLcrlCvV6nZ0dvNXYfMFgcBfYKLUBlTIZXj6Psfj1C0pO4umjB/T3BT0nu5BHV4o4lkvZcqhtbUOtSr1S9ZC+QCBAIpH430PRK9Uokv6zwJPbl1n8Po8sgPeiYXr9QdZt+LZqsLCis5B0iaddFlMuy7kyy/ltlgtVfKFQyCuvQa+LnEuqxNzkCG9jd4h//sCvH/OM9l3huj/EkgGf1lzeL+lMx01hOjNxlXc/NWZ/W3xMbOO72tVFbPIZU6+nefHqDbHHDxkOBxgdu0tkMELvtW7aTxyj/0afV3JRTD+zkUFJZzGkLKYkzpsSm0mZkiGmfOrsBfY2t9PU1sG+ljPsbznN4ZOdHGw9x4HW8zQd72TPoTZ6/AFsp+z12CqaWFYJQ0hG9eQj5CUnsYs6vksXOxgKhxi7GaGnq5vx8ftMTEwwHB1iZOgW4WAvR5uPEBLDs4XO0lIOWdFwXde720JKpiKyza6I7LP8A25MWfR/yO4fAAAAAElFTkSuQmCC'); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"inspect-element-html.png\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-c83f1.png\\\"\\n        srcset=\\\"/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-569e3.png 240w,\\n/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-93400.png 480w,\\n/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-c83f1.png 960w,\\n/static/inspect-element-html-78b977c61d1b0da26eae71cdce54f903-23e13.png 1440w\\\"\\n        sizes=\\\"(max-width: 960px) 100vw, 960px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    \\n<em>- Web Inspector Example</em></p>\\n<p>It is going to display the source of code which might have a structure such as follow:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&lt;div class=&quot;tile tile--with-overlay gutter-bottom&quot;&gt;\\n    &lt;div class=&quot;photo-tile&quot;&gt;\\n        &lt;a class=&quot;photo-tile__image-wrapper&quot;&gt;\\n            &lt;img sizes=&quot;100vw&quot; data-srcset=&quot;https:...&quot; \\n                 class&quot;tile__image js-track-photo-stat-view js-track-photo-stat-click lazyloaded&quot;&gt;</code></pre>\\n      </div>\\n<p>The HTML tags are uniquely identified by properties of tags. As a result, now, We know the location of uniquely identified data during the process of scrapping.</p>\\n<hr>\\n<h2>Getting Start with Code</h2>\\n<p>The Following is configuration need to be placed into <code class=\\\"language-text\\\">config.py</code> file. This global configuration is going to be used in different scripts.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Target dataset path\\nDATASET_PATH = &quot;./dataset&quot;\\n\\n# Fake user agent for avoiding 503 error\\nHEADERS = {\\n    &#39;User-Agent&#39;: &#39;Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.90 Safari/537.36&#39;\\n}\\n\\n# Base url of scrapping\\nBASE_URL = &quot;https://burst.shopify.com&quot;\\n\\n# Advanced parameters\\n# Categories want to scrap\\nCATEGORIES = [&quot;dog&quot;,&quot;cat&quot;]\\n\\n# Page limit to search images from URL\\nPAGE_FROM =1\\nPAGE_TO = 2\\n\\n# Number of workers for downloading pages and images for better and faster performance\\nWORKERS = 4</code></pre>\\n      </div>\\n<p>Source: <em><a href=\\\"https://github.com/zujoio/web-scrapper-python/blob/master/config.py\\\">config.py</a></em></p>\\n<p>Now, Let’s get our hand dirty with code. import some libraries to getting started with actual code:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">from bs4 import BeautifulSoup\\nimport os\\nimport urllib.request\\nfrom tqdm import tqdm\\nimport ssl</code></pre>\\n      </div>\\n<p><code class=\\\"language-text\\\">BeautifulSoup</code> is used for scraping web pages and images while <code class=\\\"language-text\\\">urllib</code> is imported to download web pages and images. <code class=\\\"language-text\\\">tqdm</code> is used for just displaying progress and <code class=\\\"language-text\\\">ssl</code> used for creating fake verification of request.</p>\\n<p>also, we need to import <code class=\\\"language-text\\\">config.py</code> file to use global configuration.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">from config import *</code></pre>\\n      </div>\\n<p>Before starting I am putting some local configuration which is going to used as below.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">timeout = 60 # Request timeout\\nurl = BASE_URL + &quot;/dog&quot; # URL being scrapped\\ntarget_dir = os.path.join(DATASET_PATH,&quot;dog&quot;) # Target directory for scrapping data</code></pre>\\n      </div>\\n<p>To download source of the page, I have used <code class=\\\"language-text\\\">urllib</code> library where <code class=\\\"language-text\\\">context</code> specifies a fake SSL certificate to avoid SSL Exceptions and <code class=\\\"language-text\\\">HEADERS</code> are imported from global configuration to avoid 503 Exception generated by the web servers.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Bypass SSL verification\\ncontext = ssl._create_unverified_context()\\n\\n# Read HTML page and save as long string\\nreq = urllib.request.Request(url, headers=HEADERS)\\nresponse = urllib.request.urlopen(req, timeout=timeout, context=context)\\n\\n# Read page source\\nhtml = response.read()</code></pre>\\n      </div>\\n<p>The downloaded HTML source code needs to be parsed to access properties of HTML tags. <code class=\\\"language-text\\\">BeautifulSoup</code> is going to use for the same context. It has well-optimized classes and methods to access HTML tags by their unique properties.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Parse HTML source using BeautifulSoup\\nsoup =  BeautifulSoup(html, &quot;html.parser&quot;)</code></pre>\\n      </div>\\n<p>Now we have a handler called <code class=\\\"language-text\\\">soup</code> which is the parsed version of HTML source code that can be attached to any supported method of <code class=\\\"language-text\\\">BeautifulSoup</code> class.</p>\\n<p>As we had learned earlier about unique tags of data. <code class=\\\"language-text\\\">soup</code> has a <code class=\\\"language-text\\\">select()</code> method which is going to help us to find specific tags from parsed HTML content. Since we have unique class of <code class=\\\"language-text\\\">&lt;Img class=&quot;js-track-photo-stat-view&quot; &gt;</code> tag.</p>\\n<p>Below piece of code is will fetch all <code class=\\\"language-text\\\">&lt;Img&gt;</code> tags from a page whose class defined as <code class=\\\"language-text\\\">js-track-photo-stat-view</code>. </p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">image_grids = soup.select(&#39;.js-track-photo-stat-view&#39;)</code></pre>\\n      </div>\\n<p>Next, we have to extract URL of images which can be helpful to download the image.</p>\\n<p><code class=\\\"language-text\\\">image_grids</code> includes multiple entries of <code class=\\\"language-text\\\">&lt;Img&gt;</code> as shown listed below. As you can see, <code class=\\\"language-text\\\">&lt;Img&gt;</code> tag has property named <code class=\\\"language-text\\\">data-srcset</code> which includes URLs of images as per resolution 1x, 2x and so on.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&lt;img alt=&quot;good morning sunshine&quot; \\nclass=&quot;tile__image js-track-photo-stat-view js-track-photo-stat-click lazyload&quot; \\ndata-category-handle=&quot;dog&quot; \\ndata-photo-id=&quot;1394&quot; \\ndata-photo-title=&quot;Good Morning Sunshine&quot; \\ndata-srcset=&quot;https://burst.shopifycdn.com/photos/good-morning-sunshine_373x.progressive.jpg 1x, https://burst.shopifycdn.com/photos/good-morning-sunshine_373x@2x.progressive.jpg 2x&quot; \\nsizes=&quot;100vw&quot; \\nsrc=&quot;https://burst.shopifycdn.com/photos/good-morning-sunshine_70x.progressive.jpg&quot;/&gt;</code></pre>\\n      </div>\\n<p>We can access properties of <code class=\\\"language-text\\\">&lt;Img&gt;</code> tag using <code class=\\\"language-text\\\">get()</code> method. Below code going to fetch the content of <code class=\\\"language-text\\\">data-srcset</code> property as well as preprocess data to find the highest resolution image from it.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">image_urls = []\\nfor image_tag in tqdm(image_grids,desc=&quot;Find Images&quot;):\\n\\n    # Fetch data tag which includes sequence of URLS\\n    image_url = image_tag.get(&#39;data-srcset&#39;)\\n\\n    # Extract highest resolution image from data tad\\n    image_url = image_url.split(&#39;,&#39;)\\n    high_resolution_pair = image_url[-1].split(&#39; &#39;)\\n    high_resolution_image_url = high_resolution_pair[1].replace(&quot;@2x&quot;, &quot;@3x&quot;)\\n\\n    # Stack all image urls\\n    image_urls.append(high_resolution_image_url)</code></pre>\\n      </div>\\n<p>Now, We have a list of URLs stacked into <code class=\\\"language-text\\\">image_urls</code> list. All we need to do is download images into targeted directories. Below script is going to do the rest.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Download images into target directory\\nfor image_url in tqdm(image_urls,desc=&quot;Download Images&quot;):\\n\\n    # Extract name of file from URL\\n    file_name = image_url.split(&quot;/&quot;)[-1]\\n\\n    # Build target path of image\\n    image_path = os.path.join(target_dir, file_name)\\n\\n    # Create directories\\n    if not os.path.exists(target_dir): os.mkdir(target_dir)\\n\\n    # Write image to file system\\n    if not os.path.exists(image_path):\\n\\n        # Read image from web\\n        req = urllib.request.Request(image_url, headers=HEADERS)\\n        response = urllib.request.urlopen(req, timeout=timeout, context=context)\\n\\n        # Write it down to file system\\n        f = open(image_path, &#39;wb&#39;)\\n        f.write(response.read())\\n        f.close()</code></pre>\\n      </div>\\n<p>Every image fetched will be downloaded in target directory.\\nThe above script is going to displays output as below which might change as per your computer’s perrformance and speed of internet!</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">Find Images: 100%|###########################################################################################################################################| 50/50 [00:00&lt;00:00, 107933.71it/s]\\nDownload Images: 100%|###########################################################################################################################################| 50/50 [00:42&lt;00:00,  2.81it/s]</code></pre>\\n      </div>\\n<p>You can find the whole script at github repo <a href=\\\"https://github.com/zujoio/web-scrapper-python/blob/master/basic_scrapper.py\\\"><code class=\\\"language-text\\\">basic_scrapper.py</code></a></p>\\n<p>You can find other methods and use of <code class=\\\"language-text\\\">BeautifulSoap</code> at official documentation from <a href=\\\"https://www.crummy.com/software/BeautifulSoup/bs4/doc/\\\">BeautifulSoup</a></p>\\n<hr>\\n<h2>Advanced Usage with Selenium Web Driver</h2>\\n<p>The problem with simple <code class=\\\"language-text\\\">urllib</code> is that It does not support to download dynamic HTML content from specified URL.</p>\\n<p>HTML going to be load after javascript executes and dynamically generate content of the page. At this moment <code class=\\\"language-text\\\">urllib</code> going to download page source but it does not include page source which required for downloading images.</p>\\n<p>For example, open any product page from <a href=\\\"https://www.amazon.com/Old-Man-Sea-Ernest-Hemingway/dp/0684801221/ref=sr_1_1?ie=UTF8&#x26;qid=1540649353&#x26;sr=8-1&#x26;keywords=old+man+and+the+sea\\\">amazon.com</a>. The whole content of the product will be generated after the javascript done. </p>\\n<p>This javascript issue can be solved using <code class=\\\"language-text\\\">selenium</code> web driver for downloading page source. It is going to open the web page autonomously and waits until every script on the page is going to be executed.</p>\\n<p>To start with <code class=\\\"language-text\\\">selenium</code>l, you have to install specific browser web driver bindings as per your system configuration using the following link. In my case, I am going to use Firefox for Mac OS.</p>\\n<h4>Official Selenium Python Package Page:</h4>\\n<p><a href=\\\"https://pypi.org/project/selenium/\\\">https://pypi.org/project/selenium/</a></p>\\n<h4>Download Firefox Web Driver Bindings:</h4>\\n<p><a href=\\\"https://github.com/mozilla/geckodriver/releases\\\">https://github.com/mozilla/geckodriver/releases</a></p>\\n<p>Install selenium package using following script</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">pip install selenium</code></pre>\\n      </div>\\n<p>Now import web driver and replace fetching page source with below code:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">from selenium import web driver\\n\\n# Use selenium firefox driver to get page source\\nbrowser = webdriver.Firefox()\\nbrowser.get(url)\\n\\n# get page source\\nhtml = browser.page_source</code></pre>\\n      </div>\\n<p>It will open an autonomous firefox window and close it after the whole page loaded successfully. You can find more use case of <code class=\\\"language-text\\\">selenium</code> from<a href=\\\"https://selenium-python.readthedocs.io/\\\">selenium-python.readthedocs.io/</a></p>\\n<hr>\\n<h2>Advanced Usage with Threading</h2>\\n<p>The above code is basic so It is not going to utilize resources available. To overcome the problem, Threading can be used to improve the performance of scrapper. Web pages and images can be downloaded parallelly using <code class=\\\"language-text\\\">ThreadPoolExecutor</code> of inbuild <code class=\\\"language-text\\\">concurrent</code> package of python.</p>\\n<p>Bolow code downloads images with array of <code class=\\\"language-text\\\">images_urls</code> using multi-threading. <code class=\\\"language-text\\\">download_image</code> is function takes single <code class=\\\"language-text\\\">url</code> of image and <code class=\\\"language-text\\\">timeout</code> of request as input. The images is going to be downloded parallely using following script.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\"># Initialize thread pool executor\\nexecutor = concurrent.futures.ThreadPoolExecutor(max_workers=5):\\n\\n# Start the load operations and mark each future with its URL\\nfuture_to_url = {executor.submit(download_image, url, timeout): url for url in image_urls}\\ncount_images = 0\\nfor future in concurrent.futures.as_completed(future_to_url):\\n    files = future.result()\\n    count_images = count_images + len(files)</code></pre>\\n      </div>\\n<p>Find out more about <code class=\\\"language-text\\\">ThreadPoolExecutor</code> from <a href=\\\"https://docs.python.org/3/library/concurrent.futures.html\\\">concurrent.futures.html</a></p>\\n<hr>\\n<h1>Advanced Usage with Scrapy</h1>\\n<p><code class=\\\"language-text\\\">Scrapy</code> can be used as advancement in scrapping. It contains rich classes and methods to extract data from multiple sources. </p>\\n<p>First of all, Install scrappy by using the following command:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">pip install scrapy</code></pre>\\n      </div>\\n<p><code class=\\\"language-text\\\">Spiders</code> are classes that you define and that <code class=\\\"language-text\\\">Scrapy</code> uses to scrape information from a website (or a group of websites). They must subclass scrapy. Spider and define the initial requests to make, optionally how to follow links in the pages, and how to parse the downloaded page content to extract data.</p>\\n<p>Let’s start with the simple example which extracts titles of a blog from multiple pages of content. </p>\\n<p>First of all, I have to import the scrapy library and implement class <code class=\\\"language-text\\\">BlogSpider</code> extended from <code class=\\\"language-text\\\">scrapy.Spider</code> base class. I have to declare the name of spider and URL using <code class=\\\"language-text\\\">name</code>  and  <code class=\\\"language-text\\\">start_urls</code> respectively. Show as below:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">import scrapy\\n\\n&quot;&quot;&quot;\\n    Abstraction of Blog Scrapper\\n&quot;&quot;&quot;\\nclass BlogSpider(scrapy.Spider):\\n    # configure name of blog spider and URL\\n    name = &#39;blogspider&#39;\\n    start_urls = [&#39;https://blog.scrapinghub.com&#39;]</code></pre>\\n      </div>\\n<p>Now, I have to override <code class=\\\"language-text\\\">parse</code> method to scrap data from the specified URL. Below code is going to find <code class=\\\"language-text\\\">&lt;h2&gt;</code> from the page whose class is <code class=\\\"language-text\\\">post-header</code> then extract the text of the first element. </p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">&quot;&quot;&quot;\\n    Implement parser \\n&quot;&quot;&quot;\\ndef parse(self, response):\\n    # extract titles from page\\n    for title in response.css(&#39;.post-header&gt;h2&#39;):\\n        yield {&#39;title&#39;: title.css(&#39;a ::text&#39;).extract_first()}</code></pre>\\n      </div>\\n<p>After that, I have called <code class=\\\"language-text\\\">parse</code> method recursively for each page in defined URL. It is much easier to work with <code class=\\\"language-text\\\">Scrapy</code> as well as much shorter code requires comparatively.</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">def parse(self, response):\\n    # extract titles from page\\n    for title in response.css(&#39;.post-header&gt;h2&#39;):\\n        yield {&#39;title&#39;: title.css(&#39;a ::text&#39;).extract_first()}\\n\\n    # move to the next page\\n    for next_page in response.css(&#39;div.prev-post &gt; a&#39;):\\n        yield response.follow(next_page, self.parse)</code></pre>\\n      </div>\\n<p>The whole code is defined as below:</p>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-text\\\"><code class=\\\"language-text\\\">import scrapy\\n\\n&quot;&quot;&quot;\\n    Abstraction of Blog Scrapper\\n&quot;&quot;&quot;\\nclass BlogSpider(scrapy.Spider):\\n    # configure name of blog spider and URL\\n    name = &#39;blogspider&#39;\\n    start_urls = [&#39;https://blog.scrapinghub.com&#39;]\\n\\n    &quot;&quot;&quot;\\n        Implement parser \\n    &quot;&quot;&quot;\\n    def parse(self, response):\\n        # extract titles from page\\n        for title in response.css(&#39;.post-header&gt;h2&#39;):\\n            yield {&#39;title&#39;: title.css(&#39;a ::text&#39;).extract_first()}\\n\\n        # move to the next page\\n        for next_page in response.css(&#39;div.prev-post &gt; a&#39;):\\n            yield response.follow(next_page, self.parse)</code></pre>\\n      </div>\\n<p>Find out more about <code class=\\\"language-text\\\">Scrapy</code> at official documentation link: <a href=\\\"https://scrapy.org/\\\">https://scrapy.org/</a></p>\\n<hr>\\n<p>Guys, that’s all from my end. You can find out code from the following repository.</p>\\n<p><a href=\\\"https://github.com/zujoio/web-scrapper-python\\\">https://github.com/zujoio/web-scrapper-python</a></p>\\n<p>Hope you like the article. If you have any questions, please feel free to leave a comment below.</p>\\n<h2>References</h2>\\n<ol>\\n<li><a href=\\\"https://www.datacamp.com/community/tutorials/web-scraping-using-python\\\">https://www.datacamp.com/community/tutorials/web-scraping-using-python</a></li>\\n<li><a href=\\\"https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/\\\">https://www.analyticsvidhya.com/blog/2015/10/beginner-guide-web-scraping-beautiful-soup-python/</a></li>\\n<li><a href=\\\"https://docs.python.org/dev/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor\\\">https://docs.python.org/dev/library/concurrent.futures.html#concurrent.futures.ThreadPoolExecutor</a></li>\\n<li><a href=\\\"https://scrapy.org/resources/\\\">https://scrapy.org/resources/</a></li>\\n<li><a href=\\\"https://docs.scrapy.org/en/latest/intro/tutorial.html\\\">https://docs.scrapy.org/en/latest/intro/tutorial.html</a></li>\\n</ol>\",\"fields\":{\"tagSlugs\":[\"/tags/web-scrapping/\",\"/tags/python/\",\"/tags/deep-learning/\",\"/tags/neural-network/\",\"/tags/machine-learning/\",\"/tags/computer-vision/\"]},\"frontmatter\":{\"title\":\"How did you scrap this data?\",\"tags\":[\"web scrapping\",\"python\",\"deep learning\",\"neural network\",\"machine learning\",\"computer vision\"],\"date\":\"2018-10-15T12:21:32.381Z\",\"description\":\"Do you just tired of doing manual stuff? Does your friends are watching some sci-fi movies and are you stuck with downloading images for your next project?  If everything above asked has a positive direction. You are at the right place!\"}}},\"pathContext\":{\"slug\":\"/posts/how-did-you-scrap-this-data/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/posts-how-did-you-scrap-this-data.json\n// module id = 434\n// module chunks = 175055536893093"],"sourceRoot":""}