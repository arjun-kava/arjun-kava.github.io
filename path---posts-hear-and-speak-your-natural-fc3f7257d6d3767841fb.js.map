{"version":3,"sources":["webpack:///path---posts-hear-and-speak-your-natural-fc3f7257d6d3767841fb.js","webpack:///./.cache/json/posts-hear-and-speak-your-natural.json"],"names":["webpackJsonp","419","module","exports","data","site","siteMetadata","title","subtitle","copyright","author","name","twitter","disqusShortname","url","markdownRemark","id","html","fields","tagSlugs","frontmatter","tags","date","description","pathContext","slug"],"mappings":"AAAAA,cAAc,iBAERC,IACA,SAAUC,EAAQC,GCHxBD,EAAAC,SAAkBC,MAAQC,MAAQC,cAAgBC,MAAA,qBAAAC,SAAA,yHAAAC,UAAA,yBAAAC,QAAgNC,KAAA,aAAAC,QAAA,cAA2CC,gBAAA,aAAAC,IAAA,kCAAuEC,gBAAmBC,GAAA,iKAAAC,KAAA;AAA60mCC,QAAsnDC,UAAA,0IAAqJC,aAAgBb,MAAA,0CAAAc,MAAA,wFAAAC,KAAA,2BAAAC,YAAA,ySAA0eC,aAAgBC,KAAA","file":"path---posts-hear-and-speak-your-natural-fc3f7257d6d3767841fb.js","sourcesContent":["webpackJsonp([136534370638616],{\n\n/***/ 419:\n/***/ (function(module, exports) {\n\n\tmodule.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Blog by Arjun Kava\",\"subtitle\":\"I am a tireless seeker of knowledge, occasional purveyor of wisdom and also, coincidentally, a deep learning engineer.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Arjun Kava\",\"twitter\":\"arjun_kava\"},\"disqusShortname\":\"arjun-kava\",\"url\":\"https://arjun-kava.github.io/\"}},\"markdownRemark\":{\"id\":\"/Volumes/multimedia/Personal-Repositories/arjunkava-blog/src/pages/articles/2018-08-17-hear-and-speak-your-natural/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<blockquote>\\n<p>“I like solitude. It is when you truly hear and speak your natural, unadulterated mind, and outcomes your most stupid self as well as your most intelligent self. It is when you realize who you are and the extents of the good and the evils which you are capable of.”\\n― Criss Jami, Killosophy</p>\\n</blockquote>\\n<p>The Human’s are evolved about <em>2.3 to 2.4 million years</em> ago. Since the 18th century, Scientists thought the great apes to be closely related to human beings. In the 19th century, They speculated that closest living relatives of humans were either <em>chimpanzees</em> or <em>gorillas</em>.</p>\\n<p>Do you know what made us different from our closest living relatives?\\nOur way of the <strong>thinking!</strong></p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-baedd.jpg\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 700px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 60%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIDAQX/xAAUAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHrzXRuIB//xAAaEAEAAgMBAAAAAAAAAAAAAAABAiEAAxES/9oACAEBAAEFAnYeifcJiFPai1//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAVEQEBAAAAAAAAAAAAAAAAAAAAIf/aAAgBAgEBPwFX/8QAGhABAAMAAwAAAAAAAAAAAAAAAQAQEQIhQf/aAAgBAQAGPwIOPeuQfHaMIV//xAAbEAADAAMBAQAAAAAAAAAAAAAAAREhMVFBYf/aAAgBAQABPyFuE2RdTY1dVDXIXCcT+EwCyxeRcORYP//aAAwDAQACAAMAAAAQcD//xAAWEQEBAQAAAAAAAAAAAAAAAAABACH/2gAIAQMBAT8QEsv/xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAgEBPxBGP//EAB4QAQACAgIDAQAAAAAAAAAAAAEAESExQVFhgZHw/9oACAEBAAE/ELFNsYIX7BLChEOCPG6uAS5AdPs0xtBX7bE6ddKw7liHMoxxP//Z'); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"1-homo-bayes.jpg\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-baedd.jpg\\\"\\n        srcset=\\\"/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-8267e.jpg 240w,\\n/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-1aec3.jpg 480w,\\n/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-baedd.jpg 700w\\\"\\n        sizes=\\\"(max-width: 700px) 100vw, 700px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    \\nImage source: <a href=\\\"https://lazarzivadinovic.blogspot.com/2016/05/parametri-spektroskopski-dvojnog-sistema.html\\\">lazarzivadinovic.blogspot.com</a></p>\\n<hr>\\n<p>Humans have a persistent process of thinking. When we read something, we understand each and every word based on our understanding of previous words also the emotions exert an effect on human thinking by producing actions such as crying, laughing and sadness. </p>\\n<p>Now the question arises, what made machines to think same as Human Being. Nowadays application such as <em>artificial assistance</em> has been understanding each and every perspective of human words.</p>\\n<hr>\\n<p>Today’s article is all about understanding, How Artificial Intelligence understands the meaning of everything we spoke. The roadmap is divided into the following major parts.</p>\\n<ol>\\n<li><em>Feed-Forward Neural Network</em></li>\\n<li><em>Recurrent Neural Network (RNN)</em></li>\\n<li><em>Falling of Recurrent Neural Network</em></li>\\n<li><em>Long-Short Term Memory (LSTM)</em></li>\\n<li><em>Simple Implementation of LSTM</em></li>\\n</ol>\\n<p>If you haven’t discovered about, How Neural Network operates then, I highly counseled reading the following articles first:</p>\\n<ol>\\n<li><em><a href=\\\"https://arjun-kava.github.io/posts/prove-simplification-of-neural-network/\\\">Prove Simplification of Artificial Neural Network - by Arjun Kava</a></em></li>\\n<li><em><a href=\\\"https://arjun-kava.github.io/posts/thats-not-enough-we-have-to-go-deeper/\\\">That’s not enough, We have to go deeper - by Arjun Kava</a></em></li>\\n</ol>\\n<p>Otherwise, Let’s start exploring.</p>\\n<hr>\\n<h3>Feed-Forward Neural Network</h3>\\n<p>The Feed-Forward Neural Network are flowing in a signal optimistic direction from Input layers then though Hidden layers to Output Layers as shown in <em>Fig 1: Feed Forward Network</em>.</p>\\n<p>As shown clearly, Feed-Forward Neural Network could not persist any memory into the network due to simple straightforward architecture. Such kind of architecture is not suitable for natural language processing which is overcome by Recurrent Neural Network (RNN).</p>\\n<p><img src=\\\"/2-feed_forward_neural_net-85ccefac262053f250b79b31f9418cd4.gif\\\" alt=\\\"2-feed_forward_neural_net.gif\\\">\\n<em>Fig 1: Feed Forward Network (Source: <a href=\\\"https://en.wikipedia.org/wiki/Feedforward_neural_network#/media/File:Feed_forward_neural_net.gif\\\">en.wikipedia.org</a>)</em></p>\\n<hr>\\n<h3>Recurrent Neural Network (RNN)</h3>\\n<p>Recurrent Neural Networks were developed in the 1980s. Hopfield networks were discovered by John Hopfield in 1982. Traditionally, Feedforward Neural Network is used to understand the meaning of words but those have not persistence property.</p>\\n<p>Suppose, You knew that we are relatives of either <em>chimpanzees</em> or <em>gorillas</em> because of our mind have persistent memory. Traditional networks could not use persist of previous memories to inform later ones.</p>\\n<p>A Feed-Forward Neural Network is constrained to accept a fixed-size vector as input and produce fixed-sized vector as output with a fixed amount of computations steps.</p>\\n<p>On the other hand, RNNs are a network of neuron-like nodes organized into successive <em>layers</em>, each node in a given layer with a directed(one-way) connection to every other node in next successive layer. Each node has time-varying real-valued activation(tanh activation). Each connection modifies real-valued weights. Nodes are either input nodes, output nodes or hidden nodes.</p>\\n<p>In simple words, <em>RNN network</em> have the ability of cycling information through the loop. The decision of the network depends on current input as well as what learned before using short-term memory.</p>\\n<p><img src=\\\"/3-recurrent_neural_network_unfold-79e174934c03239fc046bf55357ce7bd.svg\\\" alt=\\\"3-recurrent_neural_network_unfold.svg\\\">\\n<em>Fig 3: Recurrent Neural Network (Source: <a href=\\\"https://en.wikipedia.org/wiki/File:Recurrent_neural_network_unfold.svg\\\">en.wikipedia.org</a>)</em></p>\\n<p>A typical on Node looks like shown in <em>Fig 3: Recurrent Neural Network(Left)</em>, where <em>“X”</em> is input and <em>“O”</em> is output. A loop enables learning to be passed from one step of the network to the next. A recurrent neural network can be considered as the chain of multiple copies of the same network, each passing some information to a successor.</p>\\n<p>As shown in Fig 3(right portion), This represents unrolling after equal sign which accurately describes the passing of sequences for different time steps. The error back-propagates through the first to the last timestep while unrolling all the timesteps. This allows modernizing weights at each timestep of the resultant error of each timestep.</p>\\n<hr>\\n<h3>Falling of Recurrent Neural Network</h3>\\n<p>Despite, the fact that RNNs are widely used for sequential data analysis but It seems that they have certain limitations such as consider predicting the last word in the following sentence.</p>\\n<p>“A helium nucleus has two protons, whereas hydrogen has only <strong>one</strong>.”</p>\\n<p>The forward information suggests next term will be in numbers but, Due to the looping structure of RNN, The information cycles frequently which result in a mass update of weights. </p>\\n<p>This problem specifically classified into two major theories such as <em>Exploding Gradients</em> and <em>Vanishing Gradients</em>. Both enigmas are related to the gradient which is responsible for propagating an optimal solution to get the intended output.</p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-f7961.jpg\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 488px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 51.43442622950819%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB3U4pYAv/xAAXEAEBAQEAAAAAAAAAAAAAAAABABAx/9oACAEBAAEFAocY5f/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABYQAAMAAAAAAAAAAAAAAAAAAAABIP/aAAgBAQAGPwIc/wD/xAAaEAADAAMBAAAAAAAAAAAAAAAAAREQIUFh/9oACAEBAAE/IbNDPUcyRcCP/9oADAMBAAIAAwAAABDHD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQACAwEBAAAAAAAAAAAAAAEAMREhQVGB/9oACAEBAAE/EDJouV35uKEjSHYVAZo7APAvyV+z/9k='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"4-exploding-and-vanising-grandient.jpg\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-f7961.jpg\\\"\\n        srcset=\\\"/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-0336d.jpg 240w,\\n/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-0c671.jpg 480w,\\n/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-f7961.jpg 488w\\\"\\n        sizes=\\\"(max-width: 488px) 100vw, 488px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    \\n<em>Fig 4: Exploding Gradients(increasing values) and Vanising Grandients(decreasing values). (Source: <a href=\\\"http://srdas.github.io/DLBook/DL_images/TNN1.png\\\">srdas.github.io/</a>))</em></p>\\n<h4>Exploding Gradients</h4>\\n<p>Exploding gradients are an enigma where large error gradients expand and result in very large updates to neural network model weights during training.</p>\\n<p>This has the effect of your model is unstable and unable to learn from your training data.</p>\\n<h4>Vanishing Gradients</h4>\\n<p>Due to the Back-propagation, moving backward and determining gradients of loss with respect to weights. The gradient tends to set smaller and smaller as keep moving backward through the network.</p>\\n<p>Because of the lower gradient, Network holds learning or needs too much time to learn. This problem was investigated in depth by Hochreiter (1991) and Bengio, et al (1994).</p>\\n<h3>Long-Short Term Memory (LSTM)</h3>\\n<p>The Long Short-Term Memory(LSTM) was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber and improved in 2000 by Felix Gers’ team. The LSTM is the evolution of RNN which is capable of learning long-term dependencies. LSTM is normally augmented by recurrent gates.</p>\\n<p><img src=\\\"/5-Long_Short-Term_Memory-113060905cf2cc7cb2ae760f9a8cb241.svg\\\" alt=\\\"5-Long_Short-Term_Memory.svg\\\">\\n<em>Fig 5: Long Short-Term Memory(Source: <a href=\\\"https://en.wikipedia.org/wiki/File:Long_Short-Term_Memory.svg#/media/File:Long_Short-Term_Memory.svg\\\">en.wikipedia.org</a>)</em></p>\\n<p>LSTM prevents backpropagate errors from vanishing and exploding. Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier.</p>\\n<p>A common LSTM network is composed of <em>a cell</em>, <em>an input gate</em>, <em>an output gate</em> and <em>a forget gate</em>. The cell remembers values over arbitrary time intervals and three regulates the flow of information into and out of the cell.</p>\\n<p>An LSTM cell takes an input and stores it for some period of time. This is equivalent to applying the identity function ( <span class=\\\"katex\\\"><span class=\\\"katex-mathml\\\"><math><semantics><mrow><mrow><mstyle scriptlevel=\\\"0\\\" displaystyle=\\\"true\\\"><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>x</mi><mo>)</mo></mstyle></mrow></mrow><annotation encoding=\\\"application/x-tex\\\">{\\\\displaystyle f(x)=x)}</annotation></semantics></math></span><span class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"><span class=\\\"strut\\\" style=\\\"height:0.75em;\\\"></span><span class=\\\"strut bottom\\\" style=\\\"height:1em;vertical-align:-0.25em;\\\"></span><span class=\\\"base\\\"><span class=\\\"mord\\\"><span class=\\\"mord mathit\\\" style=\\\"margin-right:0.10764em;\\\">f</span><span class=\\\"mopen\\\">(</span><span class=\\\"mord mathit\\\">x</span><span class=\\\"mclose\\\">)</span><span class=\\\"mord rule\\\" style=\\\"margin-right:0.2777777777777778em;\\\"></span><span class=\\\"mrel\\\">=</span><span class=\\\"mord rule\\\" style=\\\"margin-right:0.2777777777777778em;\\\"></span><span class=\\\"mord mathit\\\">x</span><span class=\\\"mclose\\\">)</span></span></span></span></span> to the input. Because the derivative of the identity function is constant, when an LSTM network is trained with backpropagation through time, the gradient does not vanish.</p>\\n<p>The activation function of the LSTM gates is often the logistic function. Intuitively, the input gate controls the extent to which a new value flows into the cell, the forget gate controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit.</p>\\n<p>There are connections into and out of the LSTM gates, a few of which are recurrent. The weights of these connections, which need to be learned during training, determine how the gates operate. Check <em>Fig 6: Peephole Long-Short Term Memory</em> for illustration of each gate.</p>\\n<p><img src=\\\"/6-Peephole_Long_Short-Term_Memory-e1394cd86837b00056975077d21e03f8.svg\\\" alt=\\\"6-Peephole_Long_Short-Term_Memory.svg\\\">\\n<em>Fig 6: Peephole Long-Short Term Memory (Source: <a href=\\\"https://en.wikipedia.org/wiki/File:Long_Short-Term_Memory.svg#/media/File:Long_Short-Term_Memory.svg\\\">en.wikipedia.org</a>)</em></p>\\n<h3>Simple Implementation of LSTM</h3>\\n<p>Here is a simple example which classifies polarity of the sentence using IMDB polarity dataset. Keras includes inbuild dataset of IMDB polarity. </p>\\n<p><div id=\\\"gist91386052\\\" class=\\\"gist\\\">\\n    <div class=\\\"gist-file\\\">\\n      <div class=\\\"gist-data\\\">\\n        <div class=\\\"js-gist-file-update-container js-task-list-container file-box\\\">\\n  <div id=\\\"file-lstm_imdb-py\\\" class=\\\"file\\\">\\n    \\n\\n  <div itemprop=\\\"text\\\" class=\\\"blob-wrapper data type-python\\\">\\n      <table class=\\\"highlight tab-size js-file-line-container\\\" data-tab-size=\\\"8\\\">\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L1\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"1\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC1\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.datasets <span class=\\\"pl-k\\\">import</span> imdb</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L2\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"2\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC2\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.models <span class=\\\"pl-k\\\">import</span> Sequential</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L3\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"3\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC3\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.layers <span class=\\\"pl-k\\\">import</span> Dense</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L4\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"4\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC4\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.layers <span class=\\\"pl-k\\\">import</span> <span class=\\\"pl-c1\\\">LSTM</span>, Convolution1D, Flatten, Dropout</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L5\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"5\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC5\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.layers.embeddings <span class=\\\"pl-k\\\">import</span> Embedding</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L6\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"6\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC6\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.preprocessing <span class=\\\"pl-k\\\">import</span> sequence</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L7\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"7\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC7\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.callbacks <span class=\\\"pl-k\\\">import</span> TensorBoard</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L8\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"8\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC8\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.preprocessing.text <span class=\\\"pl-k\\\">import</span> text_to_word_sequence</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L9\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"9\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC9\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L10\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"10\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC10\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Using keras to load the dataset with the top_words</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L11\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"11\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC11\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">top_words <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-c1\\\">10000</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L12\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"12\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC12\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">(X_train, y_train), (X_test, y_test) <span class=\\\"pl-k\\\">=</span> imdb.load_data(<span class=\\\"pl-v\\\">num_words</span><span class=\\\"pl-k\\\">=</span>top_words)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L13\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"13\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC13\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L14\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"14\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC14\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Pad the sequence to the same length</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L15\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"15\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC15\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">max_review_length <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-c1\\\">1600</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L16\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"16\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC16\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">X_train <span class=\\\"pl-k\\\">=</span> sequence.pad_sequences(X_train, <span class=\\\"pl-v\\\">maxlen</span><span class=\\\"pl-k\\\">=</span>max_review_length)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L17\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"17\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC17\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">X_test <span class=\\\"pl-k\\\">=</span> sequence.pad_sequences(X_test, <span class=\\\"pl-v\\\">maxlen</span><span class=\\\"pl-k\\\">=</span>max_review_length)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L18\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"18\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC18\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L19\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"19\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC19\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Using embedding from Keras</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L20\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"20\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC20\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">embedding_vecor_length <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-c1\\\">300</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L21\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"21\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC21\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model <span class=\\\"pl-k\\\">=</span> Sequential()</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L22\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"22\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC22\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Embedding(top_words, embedding_vecor_length, <span class=\\\"pl-v\\\">input_length</span><span class=\\\"pl-k\\\">=</span>max_review_length))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L23\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"23\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC23\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L24\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"24\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC24\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Convolutional model (3x conv, flatten, 2x dense)</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L25\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"25\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC25\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Convolution1D(<span class=\\\"pl-c1\\\">64</span>, <span class=\\\"pl-c1\\\">3</span>, <span class=\\\"pl-v\\\">padding</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>same<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L26\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"26\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC26\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Convolution1D(<span class=\\\"pl-c1\\\">32</span>, <span class=\\\"pl-c1\\\">3</span>, <span class=\\\"pl-v\\\">padding</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>same<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L27\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"27\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC27\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Convolution1D(<span class=\\\"pl-c1\\\">16</span>, <span class=\\\"pl-c1\\\">3</span>, <span class=\\\"pl-v\\\">padding</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>same<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L28\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"28\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC28\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Flatten())</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L29\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"29\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC29\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Dropout(<span class=\\\"pl-c1\\\">0.2</span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L30\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"30\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC30\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Dense(<span class=\\\"pl-c1\\\">180</span>,<span class=\\\"pl-v\\\">activation</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>sigmoid<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L31\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"31\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC31\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Dropout(<span class=\\\"pl-c1\\\">0.2</span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L32\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"32\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC32\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Dense(<span class=\\\"pl-c1\\\">1</span>,<span class=\\\"pl-v\\\">activation</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>sigmoid<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L33\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"33\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC33\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L34\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"34\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC34\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Log to tensorboard</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L35\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"35\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC35\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">tensorBoardCallback <span class=\\\"pl-k\\\">=</span> TensorBoard(<span class=\\\"pl-v\\\">log_dir</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>./logs<span class=\\\"pl-pds\\\">&#39;</span></span>, <span class=\\\"pl-v\\\">write_graph</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-c1\\\">True</span>)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L36\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"36\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC36\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.compile(<span class=\\\"pl-v\\\">loss</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>binary_crossentropy<span class=\\\"pl-pds\\\">&#39;</span></span>, <span class=\\\"pl-v\\\">optimizer</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>adam<span class=\\\"pl-pds\\\">&#39;</span></span>, <span class=\\\"pl-v\\\">metrics</span><span class=\\\"pl-k\\\">=</span>[<span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>accuracy<span class=\\\"pl-pds\\\">&#39;</span></span>])</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L37\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"37\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC37\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L38\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"38\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC38\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.fit(X_train, y_train, <span class=\\\"pl-v\\\">epochs</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-c1\\\">3</span>, <span class=\\\"pl-v\\\">callbacks</span><span class=\\\"pl-k\\\">=</span>[tensorBoardCallback], <span class=\\\"pl-v\\\">batch_size</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-c1\\\">64</span>)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L39\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"39\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC39\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L40\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"40\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC40\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Evaluation on the test set</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L41\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"41\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC41\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">scores <span class=\\\"pl-k\\\">=</span> model.evaluate(X_test, y_test, <span class=\\\"pl-v\\\">verbose</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-c1\\\">0</span>)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L42\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"42\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC42\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c1\\\">print</span>(<span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>Accuracy: <span class=\\\"pl-c1\\\">%.2f%%</span><span class=\\\"pl-pds\\\">&quot;</span></span> <span class=\\\"pl-k\\\">%</span> (scores[<span class=\\\"pl-c1\\\">1</span>]<span class=\\\"pl-k\\\">*</span><span class=\\\"pl-c1\\\">100</span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L43\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"43\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC43\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L44\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"44\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC44\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.save(<span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>polarity_model.h5<span class=\\\"pl-pds\\\">&quot;</span></span>)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L45\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"45\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC45\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L46\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"46\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC46\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">word2index <span class=\\\"pl-k\\\">=</span> imdb.get_word_index()</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L47\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"47\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC47\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L48\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"48\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC48\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span>predict sentiment from reviews</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L49\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"49\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC49\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">bad <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>this movie was terrible and bad<span class=\\\"pl-pds\\\">&quot;</span></span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L50\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"50\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC50\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">good <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>i really liked the movie and had fun<span class=\\\"pl-pds\\\">&quot;</span></span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L51\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"51\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC51\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">for</span> review <span class=\\\"pl-k\\\">in</span> [good,bad]:</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L52\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"52\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC52\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    test<span class=\\\"pl-k\\\">=</span>[]</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L53\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"53\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC53\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    <span class=\\\"pl-k\\\">for</span> word <span class=\\\"pl-k\\\">in</span> text_to_word_sequence( <span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>i love this movie<span class=\\\"pl-pds\\\">&quot;</span></span>):</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L54\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"54\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC54\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">         test.append(word2index[word])</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L55\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"55\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC55\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L56\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"56\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC56\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    test<span class=\\\"pl-k\\\">=</span>sequence.pad_sequences([test],<span class=\\\"pl-v\\\">maxlen</span><span class=\\\"pl-k\\\">=</span>max_review_length)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L57\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"57\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC57\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    model.predict(test)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L58\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"58\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC58\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    <span class=\\\"pl-c1\\\">print</span>(<span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span><span class=\\\"pl-c1\\\">%s</span>. Sentiment: <span class=\\\"pl-c1\\\">%s</span><span class=\\\"pl-pds\\\">&quot;</span></span> <span class=\\\"pl-k\\\">%</span> (review, model.predict(test)))</td>\\n      </tr>\\n</table>\\n\\n\\n  </div>\\n\\n  </div>\\n</div>\\n\\n      </div>\\n      <div class=\\\"gist-meta\\\">\\n        <a href=\\\"https://gist.github.com/arjun-kava/4e08c8c1056785d9b0d7581d724dc08f/raw/ae3c5d528c7df1a2b62f1b2b95deb5c315737a4c/lstm_imdb.py\\\" style=\\\"float:right\\\">view raw</a>\\n        <a href=\\\"https://gist.github.com/arjun-kava/4e08c8c1056785d9b0d7581d724dc08f#file-lstm_imdb-py\\\">lstm_imdb.py</a>\\n        hosted with &#10084; by <a href=\\\"https://github.com\\\">GitHub</a>\\n      </div>\\n    </div>\\n</div>\\n</p>\\n<h3>Output</h3>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-sh\\\"><code class=\\\"language-sh\\\">Using TensorFlow backend.\\nLoading data...\\nDownloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\\n17465344/17464789 [==============================] - 62s 4us/step\\n25000 train sequences\\n25000 test sequences\\nPad sequences (samples x time)\\nx_train shape: (25000, 80)\\nx_test shape: (25000, 80)\\nBuild model...\\nTrain...\\nTrain on 25000 samples, validate on 25000 samples\\nEpoch 1/3\\n25000/25000 [==============================] - 615s 25ms/step - loss: 0.3808 - acc: 0.8203\\nEpoch 2/3\\n25000/25000 [==============================] - 623s 25ms/step - loss: 0.1679 - acc: 0.9366\\nEpoch 3/3\\n25000/25000 [==============================] - 562s 22ms/step - loss: 0.0600 - acc: 0.9794\\nAccuracy: 82.56%\\nDownloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\\n1646592/1641221 [==============================] - 6s 3us/step\\nGreat movie I had ever watched.. Sentiment: [[0.9146]]\\nyou know even better than them that you have potential! Stop portraying in parody movies!. Sentiment: [[0.03823381]]</code></pre>\\n      </div>\\n<p>As shown above, The output nearest to zero is a negative review and nearest to one is positive. That’s all for today.</p>\\n<h3>Foot Notes</h3>\\n<p>Today I had explained the major concepts used in recent Natural Langauge Processing, Hope you like it. For any queries comment into below box. I will write about more examples in the next articles till then Happy Exploring.</p>\",\"fields\":{\"tagSlugs\":[\"/tags/deep-learning/\",\"/tags/machine-learning/\",\"/tags/keras/\",\"/tags/rnn/\",\"/tags/long-short-term-memory/\",\"/tags/lstm/\",\"/tags/nlp/\"]},\"frontmatter\":{\"title\":\"Hear and Speak Your Natural - NLP keras\",\"tags\":[\"Deep Learning\",\"Machine Learning\",\"keras\",\"RNN\",\"Long Short-Term Memory\",\"LSTM\",\"NLP\"],\"date\":\"2018-08-17T17:38:55.796Z\",\"description\":\"I like solitude. It is when you truly hear and speak your natural, unadulterated mind, and outcomes your most stupid self as well as your most intelligent self. It is when you realize who you are and the extents of the good and the evils which you are capable of.”  ― Criss Jami, Killosophyi\"}}},\"pathContext\":{\"slug\":\"/posts/hear-and-speak-your-natural/\"}}\n\n/***/ })\n\n});\n\n\n// WEBPACK FOOTER //\n// path---posts-hear-and-speak-your-natural-fc3f7257d6d3767841fb.js","module.exports = {\"data\":{\"site\":{\"siteMetadata\":{\"title\":\"Blog by Arjun Kava\",\"subtitle\":\"I am a tireless seeker of knowledge, occasional purveyor of wisdom and also, coincidentally, a deep learning engineer.\",\"copyright\":\"© All rights reserved.\",\"author\":{\"name\":\"Arjun Kava\",\"twitter\":\"arjun_kava\"},\"disqusShortname\":\"arjun-kava\",\"url\":\"https://arjun-kava.github.io/\"}},\"markdownRemark\":{\"id\":\"/Volumes/multimedia/Personal-Repositories/arjunkava-blog/src/pages/articles/2018-08-17-hear-and-speak-your-natural/index.md absPath of file >>> MarkdownRemark\",\"html\":\"<blockquote>\\n<p>“I like solitude. It is when you truly hear and speak your natural, unadulterated mind, and outcomes your most stupid self as well as your most intelligent self. It is when you realize who you are and the extents of the good and the evils which you are capable of.”\\n― Criss Jami, Killosophy</p>\\n</blockquote>\\n<p>The Human’s are evolved about <em>2.3 to 2.4 million years</em> ago. Since the 18th century, Scientists thought the great apes to be closely related to human beings. In the 19th century, They speculated that closest living relatives of humans were either <em>chimpanzees</em> or <em>gorillas</em>.</p>\\n<p>Do you know what made us different from our closest living relatives?\\nOur way of the <strong>thinking!</strong></p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-baedd.jpg\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 700px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 60%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAMABQDASIAAhEBAxEB/8QAGAAAAwEBAAAAAAAAAAAAAAAAAAIDAQX/xAAUAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHrzXRuIB//xAAaEAEAAgMBAAAAAAAAAAAAAAABAiEAAxES/9oACAEBAAEFAnYeifcJiFPai1//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAVEQEBAAAAAAAAAAAAAAAAAAAAIf/aAAgBAgEBPwFX/8QAGhABAAMAAwAAAAAAAAAAAAAAAQAQEQIhQf/aAAgBAQAGPwIOPeuQfHaMIV//xAAbEAADAAMBAQAAAAAAAAAAAAAAAREhMVFBYf/aAAgBAQABPyFuE2RdTY1dVDXIXCcT+EwCyxeRcORYP//aAAwDAQACAAMAAAAQcD//xAAWEQEBAQAAAAAAAAAAAAAAAAABACH/2gAIAQMBAT8QEsv/xAAVEQEBAAAAAAAAAAAAAAAAAAABEP/aAAgBAgEBPxBGP//EAB4QAQACAgIDAQAAAAAAAAAAAAEAESExQVFhgZHw/9oACAEBAAE/ELFNsYIX7BLChEOCPG6uAS5AdPs0xtBX7bE6ddKw7liHMoxxP//Z'); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"1-homo-bayes.jpg\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-baedd.jpg\\\"\\n        srcset=\\\"/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-8267e.jpg 240w,\\n/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-1aec3.jpg 480w,\\n/static/1-homo-bayes-923eacfeb252eec8e4bb536a6d040348-baedd.jpg 700w\\\"\\n        sizes=\\\"(max-width: 700px) 100vw, 700px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    \\nImage source: <a href=\\\"https://lazarzivadinovic.blogspot.com/2016/05/parametri-spektroskopski-dvojnog-sistema.html\\\">lazarzivadinovic.blogspot.com</a></p>\\n<hr>\\n<p>Humans have a persistent process of thinking. When we read something, we understand each and every word based on our understanding of previous words also the emotions exert an effect on human thinking by producing actions such as crying, laughing and sadness. </p>\\n<p>Now the question arises, what made machines to think same as Human Being. Nowadays application such as <em>artificial assistance</em> has been understanding each and every perspective of human words.</p>\\n<hr>\\n<p>Today’s article is all about understanding, How Artificial Intelligence understands the meaning of everything we spoke. The roadmap is divided into the following major parts.</p>\\n<ol>\\n<li><em>Feed-Forward Neural Network</em></li>\\n<li><em>Recurrent Neural Network (RNN)</em></li>\\n<li><em>Falling of Recurrent Neural Network</em></li>\\n<li><em>Long-Short Term Memory (LSTM)</em></li>\\n<li><em>Simple Implementation of LSTM</em></li>\\n</ol>\\n<p>If you haven’t discovered about, How Neural Network operates then, I highly counseled reading the following articles first:</p>\\n<ol>\\n<li><em><a href=\\\"https://arjun-kava.github.io/posts/prove-simplification-of-neural-network/\\\">Prove Simplification of Artificial Neural Network - by Arjun Kava</a></em></li>\\n<li><em><a href=\\\"https://arjun-kava.github.io/posts/thats-not-enough-we-have-to-go-deeper/\\\">That’s not enough, We have to go deeper - by Arjun Kava</a></em></li>\\n</ol>\\n<p>Otherwise, Let’s start exploring.</p>\\n<hr>\\n<h3>Feed-Forward Neural Network</h3>\\n<p>The Feed-Forward Neural Network are flowing in a signal optimistic direction from Input layers then though Hidden layers to Output Layers as shown in <em>Fig 1: Feed Forward Network</em>.</p>\\n<p>As shown clearly, Feed-Forward Neural Network could not persist any memory into the network due to simple straightforward architecture. Such kind of architecture is not suitable for natural language processing which is overcome by Recurrent Neural Network (RNN).</p>\\n<p><img src=\\\"/2-feed_forward_neural_net-85ccefac262053f250b79b31f9418cd4.gif\\\" alt=\\\"2-feed_forward_neural_net.gif\\\">\\n<em>Fig 1: Feed Forward Network (Source: <a href=\\\"https://en.wikipedia.org/wiki/Feedforward_neural_network#/media/File:Feed_forward_neural_net.gif\\\">en.wikipedia.org</a>)</em></p>\\n<hr>\\n<h3>Recurrent Neural Network (RNN)</h3>\\n<p>Recurrent Neural Networks were developed in the 1980s. Hopfield networks were discovered by John Hopfield in 1982. Traditionally, Feedforward Neural Network is used to understand the meaning of words but those have not persistence property.</p>\\n<p>Suppose, You knew that we are relatives of either <em>chimpanzees</em> or <em>gorillas</em> because of our mind have persistent memory. Traditional networks could not use persist of previous memories to inform later ones.</p>\\n<p>A Feed-Forward Neural Network is constrained to accept a fixed-size vector as input and produce fixed-sized vector as output with a fixed amount of computations steps.</p>\\n<p>On the other hand, RNNs are a network of neuron-like nodes organized into successive <em>layers</em>, each node in a given layer with a directed(one-way) connection to every other node in next successive layer. Each node has time-varying real-valued activation(tanh activation). Each connection modifies real-valued weights. Nodes are either input nodes, output nodes or hidden nodes.</p>\\n<p>In simple words, <em>RNN network</em> have the ability of cycling information through the loop. The decision of the network depends on current input as well as what learned before using short-term memory.</p>\\n<p><img src=\\\"/3-recurrent_neural_network_unfold-79e174934c03239fc046bf55357ce7bd.svg\\\" alt=\\\"3-recurrent_neural_network_unfold.svg\\\">\\n<em>Fig 3: Recurrent Neural Network (Source: <a href=\\\"https://en.wikipedia.org/wiki/File:Recurrent_neural_network_unfold.svg\\\">en.wikipedia.org</a>)</em></p>\\n<p>A typical on Node looks like shown in <em>Fig 3: Recurrent Neural Network(Left)</em>, where <em>“X”</em> is input and <em>“O”</em> is output. A loop enables learning to be passed from one step of the network to the next. A recurrent neural network can be considered as the chain of multiple copies of the same network, each passing some information to a successor.</p>\\n<p>As shown in Fig 3(right portion), This represents unrolling after equal sign which accurately describes the passing of sequences for different time steps. The error back-propagates through the first to the last timestep while unrolling all the timesteps. This allows modernizing weights at each timestep of the resultant error of each timestep.</p>\\n<hr>\\n<h3>Falling of Recurrent Neural Network</h3>\\n<p>Despite, the fact that RNNs are widely used for sequential data analysis but It seems that they have certain limitations such as consider predicting the last word in the following sentence.</p>\\n<p>“A helium nucleus has two protons, whereas hydrogen has only <strong>one</strong>.”</p>\\n<p>The forward information suggests next term will be in numbers but, Due to the looping structure of RNN, The information cycles frequently which result in a mass update of weights. </p>\\n<p>This problem specifically classified into two major theories such as <em>Exploding Gradients</em> and <em>Vanishing Gradients</em>. Both enigmas are related to the gradient which is responsible for propagating an optimal solution to get the intended output.</p>\\n<p>\\n  <a\\n    class=\\\"gatsby-resp-image-link\\\"\\n    href=\\\"/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-f7961.jpg\\\"\\n    style=\\\"display: block\\\"\\n    target=\\\"_blank\\\"\\n    rel=\\\"noopener\\\"\\n  >\\n  \\n  <span\\n    class=\\\"gatsby-resp-image-wrapper\\\"\\n    style=\\\"position: relative; display: block; ; max-width: 488px; margin-left: auto; margin-right: auto;\\\"\\n  >\\n    <span\\n      class=\\\"gatsby-resp-image-background-image\\\"\\n      style=\\\"padding-bottom: 51.43442622950819%; position: relative; bottom: 0; left: 0; background-image: url('data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGAAAAgMAAAAAAAAAAAAAAAAAAAECAwX/xAAVAQEBAAAAAAAAAAAAAAAAAAAAAf/aAAwDAQACEAMQAAAB3U4pYAv/xAAXEAEBAQEAAAAAAAAAAAAAAAABABAx/9oACAEBAAEFAocY5f/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABYQAAMAAAAAAAAAAAAAAAAAAAABIP/aAAgBAQAGPwIc/wD/xAAaEAADAAMBAAAAAAAAAAAAAAAAAREQIUFh/9oACAEBAAE/IbNDPUcyRcCP/9oADAMBAAIAAwAAABDHD//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8QP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABsQAQACAwEBAAAAAAAAAAAAAAEAMREhQVGB/9oACAEBAAE/EDJouV35uKEjSHYVAZo7APAvyV+z/9k='); background-size: cover; display: block;\\\"\\n    >\\n      <img\\n        class=\\\"gatsby-resp-image-image\\\"\\n        style=\\\"width: 100%; height: 100%; margin: 0; vertical-align: middle; position: absolute; top: 0; left: 0; box-shadow: inset 0px 0px 0px 400px white;\\\"\\n        alt=\\\"4-exploding-and-vanising-grandient.jpg\\\"\\n        title=\\\"\\\"\\n        src=\\\"/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-f7961.jpg\\\"\\n        srcset=\\\"/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-0336d.jpg 240w,\\n/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-0c671.jpg 480w,\\n/static/4-exploding-and-vanising-grandient-0a36dcaf3fafc67dd44806ac53543411-f7961.jpg 488w\\\"\\n        sizes=\\\"(max-width: 488px) 100vw, 488px\\\"\\n      />\\n    </span>\\n  </span>\\n  \\n  </a>\\n    \\n<em>Fig 4: Exploding Gradients(increasing values) and Vanising Grandients(decreasing values). (Source: <a href=\\\"http://srdas.github.io/DLBook/DL_images/TNN1.png\\\">srdas.github.io/</a>))</em></p>\\n<h4>Exploding Gradients</h4>\\n<p>Exploding gradients are an enigma where large error gradients expand and result in very large updates to neural network model weights during training.</p>\\n<p>This has the effect of your model is unstable and unable to learn from your training data.</p>\\n<h4>Vanishing Gradients</h4>\\n<p>Due to the Back-propagation, moving backward and determining gradients of loss with respect to weights. The gradient tends to set smaller and smaller as keep moving backward through the network.</p>\\n<p>Because of the lower gradient, Network holds learning or needs too much time to learn. This problem was investigated in depth by Hochreiter (1991) and Bengio, et al (1994).</p>\\n<h3>Long-Short Term Memory (LSTM)</h3>\\n<p>The Long Short-Term Memory(LSTM) was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber and improved in 2000 by Felix Gers’ team. The LSTM is the evolution of RNN which is capable of learning long-term dependencies. LSTM is normally augmented by recurrent gates.</p>\\n<p><img src=\\\"/5-Long_Short-Term_Memory-113060905cf2cc7cb2ae760f9a8cb241.svg\\\" alt=\\\"5-Long_Short-Term_Memory.svg\\\">\\n<em>Fig 5: Long Short-Term Memory(Source: <a href=\\\"https://en.wikipedia.org/wiki/File:Long_Short-Term_Memory.svg#/media/File:Long_Short-Term_Memory.svg\\\">en.wikipedia.org</a>)</em></p>\\n<p>LSTM prevents backpropagate errors from vanishing and exploding. Instead, errors can flow backward through unlimited numbers of virtual layers unfolded in space. LSTM can learn tasks that require memories of events that happened thousands or even millions of discrete time steps earlier.</p>\\n<p>A common LSTM network is composed of <em>a cell</em>, <em>an input gate</em>, <em>an output gate</em> and <em>a forget gate</em>. The cell remembers values over arbitrary time intervals and three regulates the flow of information into and out of the cell.</p>\\n<p>An LSTM cell takes an input and stores it for some period of time. This is equivalent to applying the identity function ( <span class=\\\"katex\\\"><span class=\\\"katex-mathml\\\"><math><semantics><mrow><mrow><mstyle scriptlevel=\\\"0\\\" displaystyle=\\\"true\\\"><mi>f</mi><mo>(</mo><mi>x</mi><mo>)</mo><mo>=</mo><mi>x</mi><mo>)</mo></mstyle></mrow></mrow><annotation encoding=\\\"application/x-tex\\\">{\\\\displaystyle f(x)=x)}</annotation></semantics></math></span><span class=\\\"katex-html\\\" aria-hidden=\\\"true\\\"><span class=\\\"strut\\\" style=\\\"height:0.75em;\\\"></span><span class=\\\"strut bottom\\\" style=\\\"height:1em;vertical-align:-0.25em;\\\"></span><span class=\\\"base\\\"><span class=\\\"mord\\\"><span class=\\\"mord mathit\\\" style=\\\"margin-right:0.10764em;\\\">f</span><span class=\\\"mopen\\\">(</span><span class=\\\"mord mathit\\\">x</span><span class=\\\"mclose\\\">)</span><span class=\\\"mord rule\\\" style=\\\"margin-right:0.2777777777777778em;\\\"></span><span class=\\\"mrel\\\">=</span><span class=\\\"mord rule\\\" style=\\\"margin-right:0.2777777777777778em;\\\"></span><span class=\\\"mord mathit\\\">x</span><span class=\\\"mclose\\\">)</span></span></span></span></span> to the input. Because the derivative of the identity function is constant, when an LSTM network is trained with backpropagation through time, the gradient does not vanish.</p>\\n<p>The activation function of the LSTM gates is often the logistic function. Intuitively, the input gate controls the extent to which a new value flows into the cell, the forget gate controls the extent to which a value remains in the cell and the output gate controls the extent to which the value in the cell is used to compute the output activation of the LSTM unit.</p>\\n<p>There are connections into and out of the LSTM gates, a few of which are recurrent. The weights of these connections, which need to be learned during training, determine how the gates operate. Check <em>Fig 6: Peephole Long-Short Term Memory</em> for illustration of each gate.</p>\\n<p><img src=\\\"/6-Peephole_Long_Short-Term_Memory-e1394cd86837b00056975077d21e03f8.svg\\\" alt=\\\"6-Peephole_Long_Short-Term_Memory.svg\\\">\\n<em>Fig 6: Peephole Long-Short Term Memory (Source: <a href=\\\"https://en.wikipedia.org/wiki/File:Long_Short-Term_Memory.svg#/media/File:Long_Short-Term_Memory.svg\\\">en.wikipedia.org</a>)</em></p>\\n<h3>Simple Implementation of LSTM</h3>\\n<p>Here is a simple example which classifies polarity of the sentence using IMDB polarity dataset. Keras includes inbuild dataset of IMDB polarity. </p>\\n<p><div id=\\\"gist91386052\\\" class=\\\"gist\\\">\\n    <div class=\\\"gist-file\\\">\\n      <div class=\\\"gist-data\\\">\\n        <div class=\\\"js-gist-file-update-container js-task-list-container file-box\\\">\\n  <div id=\\\"file-lstm_imdb-py\\\" class=\\\"file\\\">\\n    \\n\\n  <div itemprop=\\\"text\\\" class=\\\"blob-wrapper data type-python\\\">\\n      <table class=\\\"highlight tab-size js-file-line-container\\\" data-tab-size=\\\"8\\\">\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L1\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"1\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC1\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.datasets <span class=\\\"pl-k\\\">import</span> imdb</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L2\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"2\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC2\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.models <span class=\\\"pl-k\\\">import</span> Sequential</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L3\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"3\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC3\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.layers <span class=\\\"pl-k\\\">import</span> Dense</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L4\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"4\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC4\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.layers <span class=\\\"pl-k\\\">import</span> <span class=\\\"pl-c1\\\">LSTM</span>, Convolution1D, Flatten, Dropout</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L5\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"5\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC5\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.layers.embeddings <span class=\\\"pl-k\\\">import</span> Embedding</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L6\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"6\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC6\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.preprocessing <span class=\\\"pl-k\\\">import</span> sequence</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L7\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"7\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC7\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.callbacks <span class=\\\"pl-k\\\">import</span> TensorBoard</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L8\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"8\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC8\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">from</span> keras.preprocessing.text <span class=\\\"pl-k\\\">import</span> text_to_word_sequence</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L9\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"9\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC9\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L10\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"10\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC10\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Using keras to load the dataset with the top_words</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L11\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"11\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC11\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">top_words <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-c1\\\">10000</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L12\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"12\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC12\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">(X_train, y_train), (X_test, y_test) <span class=\\\"pl-k\\\">=</span> imdb.load_data(<span class=\\\"pl-v\\\">num_words</span><span class=\\\"pl-k\\\">=</span>top_words)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L13\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"13\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC13\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L14\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"14\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC14\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Pad the sequence to the same length</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L15\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"15\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC15\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">max_review_length <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-c1\\\">1600</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L16\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"16\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC16\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">X_train <span class=\\\"pl-k\\\">=</span> sequence.pad_sequences(X_train, <span class=\\\"pl-v\\\">maxlen</span><span class=\\\"pl-k\\\">=</span>max_review_length)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L17\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"17\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC17\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">X_test <span class=\\\"pl-k\\\">=</span> sequence.pad_sequences(X_test, <span class=\\\"pl-v\\\">maxlen</span><span class=\\\"pl-k\\\">=</span>max_review_length)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L18\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"18\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC18\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L19\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"19\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC19\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Using embedding from Keras</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L20\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"20\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC20\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">embedding_vecor_length <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-c1\\\">300</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L21\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"21\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC21\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model <span class=\\\"pl-k\\\">=</span> Sequential()</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L22\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"22\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC22\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Embedding(top_words, embedding_vecor_length, <span class=\\\"pl-v\\\">input_length</span><span class=\\\"pl-k\\\">=</span>max_review_length))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L23\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"23\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC23\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L24\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"24\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC24\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Convolutional model (3x conv, flatten, 2x dense)</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L25\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"25\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC25\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Convolution1D(<span class=\\\"pl-c1\\\">64</span>, <span class=\\\"pl-c1\\\">3</span>, <span class=\\\"pl-v\\\">padding</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>same<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L26\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"26\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC26\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Convolution1D(<span class=\\\"pl-c1\\\">32</span>, <span class=\\\"pl-c1\\\">3</span>, <span class=\\\"pl-v\\\">padding</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>same<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L27\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"27\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC27\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Convolution1D(<span class=\\\"pl-c1\\\">16</span>, <span class=\\\"pl-c1\\\">3</span>, <span class=\\\"pl-v\\\">padding</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>same<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L28\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"28\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC28\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Flatten())</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L29\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"29\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC29\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Dropout(<span class=\\\"pl-c1\\\">0.2</span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L30\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"30\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC30\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Dense(<span class=\\\"pl-c1\\\">180</span>,<span class=\\\"pl-v\\\">activation</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>sigmoid<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L31\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"31\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC31\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Dropout(<span class=\\\"pl-c1\\\">0.2</span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L32\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"32\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC32\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.add(Dense(<span class=\\\"pl-c1\\\">1</span>,<span class=\\\"pl-v\\\">activation</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>sigmoid<span class=\\\"pl-pds\\\">&#39;</span></span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L33\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"33\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC33\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L34\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"34\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC34\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Log to tensorboard</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L35\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"35\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC35\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">tensorBoardCallback <span class=\\\"pl-k\\\">=</span> TensorBoard(<span class=\\\"pl-v\\\">log_dir</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>./logs<span class=\\\"pl-pds\\\">&#39;</span></span>, <span class=\\\"pl-v\\\">write_graph</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-c1\\\">True</span>)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L36\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"36\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC36\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.compile(<span class=\\\"pl-v\\\">loss</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>binary_crossentropy<span class=\\\"pl-pds\\\">&#39;</span></span>, <span class=\\\"pl-v\\\">optimizer</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>adam<span class=\\\"pl-pds\\\">&#39;</span></span>, <span class=\\\"pl-v\\\">metrics</span><span class=\\\"pl-k\\\">=</span>[<span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&#39;</span>accuracy<span class=\\\"pl-pds\\\">&#39;</span></span>])</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L37\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"37\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC37\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L38\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"38\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC38\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.fit(X_train, y_train, <span class=\\\"pl-v\\\">epochs</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-c1\\\">3</span>, <span class=\\\"pl-v\\\">callbacks</span><span class=\\\"pl-k\\\">=</span>[tensorBoardCallback], <span class=\\\"pl-v\\\">batch_size</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-c1\\\">64</span>)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L39\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"39\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC39\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L40\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"40\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC40\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span> Evaluation on the test set</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L41\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"41\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC41\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">scores <span class=\\\"pl-k\\\">=</span> model.evaluate(X_test, y_test, <span class=\\\"pl-v\\\">verbose</span><span class=\\\"pl-k\\\">=</span><span class=\\\"pl-c1\\\">0</span>)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L42\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"42\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC42\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c1\\\">print</span>(<span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>Accuracy: <span class=\\\"pl-c1\\\">%.2f%%</span><span class=\\\"pl-pds\\\">&quot;</span></span> <span class=\\\"pl-k\\\">%</span> (scores[<span class=\\\"pl-c1\\\">1</span>]<span class=\\\"pl-k\\\">*</span><span class=\\\"pl-c1\\\">100</span>))</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L43\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"43\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC43\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L44\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"44\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC44\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">model.save(<span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>polarity_model.h5<span class=\\\"pl-pds\\\">&quot;</span></span>)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L45\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"45\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC45\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L46\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"46\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC46\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">word2index <span class=\\\"pl-k\\\">=</span> imdb.get_word_index()</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L47\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"47\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC47\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L48\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"48\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC48\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-c\\\"><span class=\\\"pl-c\\\">#</span>predict sentiment from reviews</span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L49\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"49\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC49\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">bad <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>this movie was terrible and bad<span class=\\\"pl-pds\\\">&quot;</span></span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L50\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"50\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC50\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">good <span class=\\\"pl-k\\\">=</span> <span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>i really liked the movie and had fun<span class=\\\"pl-pds\\\">&quot;</span></span></td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L51\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"51\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC51\\\" class=\\\"blob-code blob-code-inner js-file-line\\\"><span class=\\\"pl-k\\\">for</span> review <span class=\\\"pl-k\\\">in</span> [good,bad]:</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L52\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"52\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC52\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    test<span class=\\\"pl-k\\\">=</span>[]</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L53\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"53\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC53\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    <span class=\\\"pl-k\\\">for</span> word <span class=\\\"pl-k\\\">in</span> text_to_word_sequence( <span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span>i love this movie<span class=\\\"pl-pds\\\">&quot;</span></span>):</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L54\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"54\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC54\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">         test.append(word2index[word])</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L55\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"55\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC55\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">\\n</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L56\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"56\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC56\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    test<span class=\\\"pl-k\\\">=</span>sequence.pad_sequences([test],<span class=\\\"pl-v\\\">maxlen</span><span class=\\\"pl-k\\\">=</span>max_review_length)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L57\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"57\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC57\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    model.predict(test)</td>\\n      </tr>\\n      <tr>\\n        <td id=\\\"file-lstm_imdb-py-L58\\\" class=\\\"blob-num js-line-number\\\" data-line-number=\\\"58\\\"></td>\\n        <td id=\\\"file-lstm_imdb-py-LC58\\\" class=\\\"blob-code blob-code-inner js-file-line\\\">    <span class=\\\"pl-c1\\\">print</span>(<span class=\\\"pl-s\\\"><span class=\\\"pl-pds\\\">&quot;</span><span class=\\\"pl-c1\\\">%s</span>. Sentiment: <span class=\\\"pl-c1\\\">%s</span><span class=\\\"pl-pds\\\">&quot;</span></span> <span class=\\\"pl-k\\\">%</span> (review, model.predict(test)))</td>\\n      </tr>\\n</table>\\n\\n\\n  </div>\\n\\n  </div>\\n</div>\\n\\n      </div>\\n      <div class=\\\"gist-meta\\\">\\n        <a href=\\\"https://gist.github.com/arjun-kava/4e08c8c1056785d9b0d7581d724dc08f/raw/ae3c5d528c7df1a2b62f1b2b95deb5c315737a4c/lstm_imdb.py\\\" style=\\\"float:right\\\">view raw</a>\\n        <a href=\\\"https://gist.github.com/arjun-kava/4e08c8c1056785d9b0d7581d724dc08f#file-lstm_imdb-py\\\">lstm_imdb.py</a>\\n        hosted with &#10084; by <a href=\\\"https://github.com\\\">GitHub</a>\\n      </div>\\n    </div>\\n</div>\\n</p>\\n<h3>Output</h3>\\n<div class=\\\"gatsby-highlight\\\">\\n      <pre class=\\\"language-sh\\\"><code class=\\\"language-sh\\\">Using TensorFlow backend.\\nLoading data...\\nDownloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\\n17465344/17464789 [==============================] - 62s 4us/step\\n25000 train sequences\\n25000 test sequences\\nPad sequences (samples x time)\\nx_train shape: (25000, 80)\\nx_test shape: (25000, 80)\\nBuild model...\\nTrain...\\nTrain on 25000 samples, validate on 25000 samples\\nEpoch 1/3\\n25000/25000 [==============================] - 615s 25ms/step - loss: 0.3808 - acc: 0.8203\\nEpoch 2/3\\n25000/25000 [==============================] - 623s 25ms/step - loss: 0.1679 - acc: 0.9366\\nEpoch 3/3\\n25000/25000 [==============================] - 562s 22ms/step - loss: 0.0600 - acc: 0.9794\\nAccuracy: 82.56%\\nDownloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\\n1646592/1641221 [==============================] - 6s 3us/step\\nGreat movie I had ever watched.. Sentiment: [[0.9146]]\\nyou know even better than them that you have potential! Stop portraying in parody movies!. Sentiment: [[0.03823381]]</code></pre>\\n      </div>\\n<p>As shown above, The output nearest to zero is a negative review and nearest to one is positive. That’s all for today.</p>\\n<h3>Foot Notes</h3>\\n<p>Today I had explained the major concepts used in recent Natural Langauge Processing, Hope you like it. For any queries comment into below box. I will write about more examples in the next articles till then Happy Exploring.</p>\",\"fields\":{\"tagSlugs\":[\"/tags/deep-learning/\",\"/tags/machine-learning/\",\"/tags/keras/\",\"/tags/rnn/\",\"/tags/long-short-term-memory/\",\"/tags/lstm/\",\"/tags/nlp/\"]},\"frontmatter\":{\"title\":\"Hear and Speak Your Natural - NLP keras\",\"tags\":[\"Deep Learning\",\"Machine Learning\",\"keras\",\"RNN\",\"Long Short-Term Memory\",\"LSTM\",\"NLP\"],\"date\":\"2018-08-17T17:38:55.796Z\",\"description\":\"I like solitude. It is when you truly hear and speak your natural, unadulterated mind, and outcomes your most stupid self as well as your most intelligent self. It is when you realize who you are and the extents of the good and the evils which you are capable of.”  ― Criss Jami, Killosophyi\"}}},\"pathContext\":{\"slug\":\"/posts/hear-and-speak-your-natural/\"}}\n\n\n//////////////////\n// WEBPACK FOOTER\n// ./~/json-loader!./.cache/json/posts-hear-and-speak-your-natural.json\n// module id = 419\n// module chunks = 136534370638616"],"sourceRoot":""}